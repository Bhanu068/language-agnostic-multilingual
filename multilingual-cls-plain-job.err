Using custom data configuration default-language=en
Reusing dataset xnli (/home/bhanuv/.cache/huggingface/datasets/xnli/default-language=en/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd)
Some weights of the model checkpoint at facebook/m2m100_418M were not used when initializing M2M100Encoder: ['model.encoder.layers.0.fc2.weight', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.10.encoder_attn.out_proj.weight', 'model.encoder.layer_norm.bias', 'model.encoder.layers.7.fc2.weight', 'model.decoder.layers.8.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.5.fc1.bias', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.7.fc1.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.4.fc1.weight', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layer_norm.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.9.encoder_attn.v_proj.weight', 'model.decoder.layers.6.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.8.encoder_attn.q_proj.weight', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.10.fc1.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.8.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.9.fc2.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.encoder_attn.k_proj.weight', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.6.encoder_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.6.encoder_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'lm_head.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.fc2.weight', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.encoder.layers.4.fc1.weight', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.3.fc2.bias', 'model.encoder.layers.8.fc2.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.decoder.layers.3.fc1.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.k_proj.weight', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.11.encoder_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.9.fc2.bias', 'model.decoder.layers.11.fc1.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.9.encoder_attn_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layer_norm.weight', 'model.decoder.layers.11.encoder_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.10.fc1.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.decoder.layers.1.fc2.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.encoder.layers.6.fc1.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.10.encoder_attn_layer_norm.weight', 'model.decoder.layers.11.encoder_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.8.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.9.encoder_attn.q_proj.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.4.fc2.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.encoder_attn.out_proj.weight', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc2.bias', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.11.encoder_attn_layer_norm.weight', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.10.encoder_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.10.encoder_attn.q_proj.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.8.encoder_attn.v_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.encoder.layers.5.fc1.weight', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.2.fc2.bias', 'model.shared.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.7.fc2.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.0.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn_layer_norm.bias', 'model.decoder.layers.9.encoder_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.9.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.7.encoder_attn.v_proj.weight', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.7.encoder_attn.q_proj.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.10.fc1.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layer_norm.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.10.encoder_attn.k_proj.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.8.fc1.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.8.fc2.bias', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.0.fc1.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.2.self_attn.q_proj.bias']
- This IS expected if you are initializing M2M100Encoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing M2M100Encoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of M2M100Encoder were not initialized from the model checkpoint at facebook/m2m100_418M and are newly initialized: ['model.layers.0.self_attn.out_proj.bias', 'model.layers.7.final_layer_norm.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.11.self_attn.out_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.8.self_attn_layer_norm.weight', 'model.embed_positions.weights', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.fc2.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.3.self_attn_layer_norm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn_layer_norm.weight', 'model.layers.10.self_attn.out_proj.weight', 'model.layers.9.self_attn.out_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.6.final_layer_norm.weight', 'model.layers.9.final_layer_norm.weight', 'model.layers.1.self_attn.out_proj.weight', 'model.layers.3.fc2.bias', 'model.layers.8.fc1.weight', 'model.layers.11.fc1.weight', 'model.layers.11.fc1.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.0.self_attn_layer_norm.bias', 'model.layers.6.fc1.weight', 'model.layers.6.self_attn_layer_norm.weight', 'model.layers.0.fc1.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.10.self_attn_layer_norm.weight', 'model.layers.10.self_attn_layer_norm.bias', 'model.layers.2.final_layer_norm.bias', 'model.layers.8.final_layer_norm.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.2.self_attn.out_proj.bias', 'model.layers.4.self_attn_layer_norm.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.3.self_attn.out_proj.weight', 'model.layers.1.fc1.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.3.self_attn.out_proj.bias', 'model.layers.1.fc2.bias', 'model.layers.9.fc1.bias', 'model.layers.3.self_attn_layer_norm.bias', 'model.layers.11.self_attn_layer_norm.weight', 'model.layers.5.self_attn_layer_norm.weight', 'model.layers.9.self_attn.out_proj.weight', 'model.layers.0.fc2.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.6.fc2.bias', 'model.layers.6.fc1.bias', 'model.layers.0.final_layer_norm.bias', 'model.layers.4.self_attn.out_proj.weight', 'model.layers.7.fc1.bias', 'model.layers.2.self_attn_layer_norm.weight', 'model.layers.5.fc2.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.11.self_attn.out_proj.weight', 'model.layers.1.self_attn_layer_norm.bias', 'model.layers.9.self_attn_layer_norm.weight', 'model.layer_norm.weight', 'model.layers.3.final_layer_norm.bias', 'model.layers.10.fc1.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.2.fc1.weight', 'model.layers.7.fc2.bias', 'model.layers.7.final_layer_norm.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.9.self_attn_layer_norm.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.2.fc2.bias', 'model.layers.3.fc1.weight', 'model.layers.9.final_layer_norm.bias', 'model.layers.11.self_attn_layer_norm.bias', 'model.layers.2.final_layer_norm.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.3.fc1.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.0.self_attn.out_proj.weight', 'model.layers.0.fc1.weight', 'model.layers.2.fc2.weight', 'model.layers.1.fc1.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.5.fc1.weight', 'model.layers.10.final_layer_norm.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.1.self_attn.out_proj.bias', 'model.layers.2.self_attn_layer_norm.bias', 'model.layers.7.self_attn.out_proj.weight', 'model.layers.5.final_layer_norm.bias', 'model.layers.4.fc1.weight', 'model.layers.4.fc2.weight', 'model.layers.9.fc1.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.out_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn_layer_norm.bias', 'model.layers.10.self_attn.out_proj.bias', 'model.layers.8.fc2.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.9.fc2.weight', 'model.layers.5.fc1.bias', 'model.layers.7.self_attn_layer_norm.bias', 'model.layers.0.final_layer_norm.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.6.self_attn_layer_norm.bias', 'model.layers.10.fc2.bias', 'model.layers.6.fc2.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.6.final_layer_norm.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.3.final_layer_norm.weight', 'model.layer_norm.bias', 'model.layers.3.fc2.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.2.fc1.bias', 'model.layers.1.fc2.weight', 'model.layers.6.self_attn.out_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn_layer_norm.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.9.fc2.bias', 'model.layers.5.final_layer_norm.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.4.self_attn_layer_norm.weight', 'model.layers.8.self_attn_layer_norm.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.6.self_attn.out_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.1.final_layer_norm.bias', 'model.layers.4.self_attn.out_proj.bias', 'model.layers.4.final_layer_norm.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.1.final_layer_norm.weight', 'model.layers.5.fc2.bias', 'model.layers.8.fc1.bias', 'model.layers.4.final_layer_norm.weight', 'model.layers.8.self_attn.out_proj.bias', 'model.layers.8.final_layer_norm.bias', 'model.layers.11.final_layer_norm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.4.fc2.bias', 'model.layers.10.fc2.weight', 'model.layers.5.self_attn.out_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.11.final_layer_norm.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.7.fc1.weight', 'model.layers.2.self_attn.out_proj.weight', 'model.layers.8.fc2.bias', 'model.layers.0.self_attn_layer_norm.weight', 'model.embed_tokens.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.11.fc2.weight', 'model.layers.5.self_attn.out_proj.weight', 'model.layers.0.fc2.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.fc1.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.4.fc1.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.out_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.7.fc2.weight', 'model.layers.10.final_layer_norm.weight', 'model.layers.4.self_attn.k_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /home/bhanuv/.cache/huggingface/datasets/xnli/default-language=en/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd/cache-215151111e50e1cd.arrow
Using amp half precision backend
The following columns in the training set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 2490
  Num Epochs = 80
  Instantaneous batch size per device = 128
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 1600
  0%|          | 0/1600 [00:00<?, ?it/s]  0%|          | 1/1600 [00:00<06:20,  4.20it/s]  0%|          | 4/1600 [00:00<02:08, 12.43it/s]  0%|          | 7/1600 [00:00<01:36, 16.58it/s]  1%|          | 10/1600 [00:00<01:22, 19.23it/s]  1%|          | 13/1600 [00:00<01:14, 21.16it/s]  1%|          | 16/1600 [00:00<01:11, 22.08it/s]  1%|          | 19/1600 [00:00<01:08, 23.16it/s]                                                   1%|▏         | 20/1600 [00:01<01:08, 23.16it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.87it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.36it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.06it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.43it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.38it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A                                                 
                                               [A  1%|▏         | 20/1600 [00:01<01:08, 23.16it/s]
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-20
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-20/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-20/special_tokens_map.json
  1%|▏         | 22/1600 [00:05<13:37,  1.93it/s]  2%|▏         | 25/1600 [00:05<09:40,  2.71it/s]  2%|▏         | 28/1600 [00:05<06:59,  3.75it/s]  2%|▏         | 31/1600 [00:05<05:10,  5.05it/s]  2%|▏         | 34/1600 [00:06<03:55,  6.64it/s]  2%|▏         | 37/1600 [00:06<03:04,  8.47it/s]  2%|▎         | 40/1600 [00:06<02:24, 10.82it/s]                                                   2%|▎         | 40/1600 [00:06<02:24, 10.82it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.07it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.48it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.83it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.45it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.35it/s][A                                                 
                                               [A  2%|▎         | 40/1600 [00:07<02:24, 10.82it/s]
100%|██████████| 20/20 [00:00<00:00, 26.35it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-40
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-40/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-40/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-20] due to args.save_total_limit
  3%|▎         | 43/1600 [00:10<13:48,  1.88it/s]  3%|▎         | 45/1600 [00:11<10:59,  2.36it/s]  3%|▎         | 48/1600 [00:11<07:46,  3.33it/s]  3%|▎         | 51/1600 [00:11<05:39,  4.56it/s]  3%|▎         | 54/1600 [00:11<04:12,  6.11it/s]  4%|▎         | 57/1600 [00:11<03:14,  7.93it/s]  4%|▍         | 60/1600 [00:11<02:32, 10.13it/s]                                                   4%|▍         | 60/1600 [00:11<02:32, 10.13it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.11it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.50it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.86it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.45it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.34it/s][A/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                 
                                               [A  4%|▍         | 60/1600 [00:12<02:32, 10.13it/s]
100%|██████████| 20/20 [00:00<00:00, 26.34it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-60
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-60/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-60/special_tokens_map.json
  4%|▍         | 63/1600 [00:16<13:21,  1.92it/s]  4%|▍         | 66/1600 [00:16<09:37,  2.65it/s]  4%|▍         | 69/1600 [00:16<07:02,  3.62it/s]  4%|▍         | 72/1600 [00:16<05:15,  4.85it/s]  5%|▍         | 75/1600 [00:16<03:59,  6.37it/s]  5%|▍         | 78/1600 [00:16<03:06,  8.16it/s]                                                   5%|▌         | 80/1600 [00:16<03:06,  8.16it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.12it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.40it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.07it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.45it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.39it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A                                                 
                                               [A  5%|▌         | 80/1600 [00:17<03:06,  8.16it/s]
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-80
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-80/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-80/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-60] due to args.save_total_limit
  5%|▌         | 81/1600 [00:21<14:15,  1.78it/s]  5%|▌         | 84/1600 [00:21<10:18,  2.45it/s]  5%|▌         | 87/1600 [00:21<07:32,  3.34it/s]  6%|▌         | 90/1600 [00:21<05:34,  4.52it/s]  6%|▌         | 93/1600 [00:22<04:12,  5.97it/s]  6%|▌         | 96/1600 [00:22<03:16,  7.65it/s]  6%|▌         | 99/1600 [00:22<02:35,  9.65it/s]                                                   6%|▋         | 100/1600 [00:22<02:35,  9.65it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.45it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.77it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.39it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.34it/s][A                                                  
                                               [A  6%|▋         | 100/1600 [00:23<02:35,  9.65it/s]
100%|██████████| 20/20 [00:00<00:00, 26.34it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-100
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-100/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-100/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-80] due to args.save_total_limit
  6%|▋         | 102/1600 [00:26<13:12,  1.89it/s]  7%|▋         | 105/1600 [00:26<09:34,  2.60it/s]  7%|▋         | 108/1600 [00:27<06:58,  3.56it/s]  7%|▋         | 111/1600 [00:27<05:10,  4.80it/s]  7%|▋         | 114/1600 [00:27<03:55,  6.31it/s]  7%|▋         | 117/1600 [00:27<03:03,  8.07it/s]  8%|▊         | 120/1600 [00:27<02:23, 10.32it/s]                                                    8%|▊         | 120/1600 [00:27<02:23, 10.32it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.96it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.42it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.08it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.46it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.46it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.34it/s][A                                                  
                                               [A  8%|▊         | 120/1600 [00:28<02:23, 10.32it/s]
100%|██████████| 20/20 [00:00<00:00, 27.34it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-120
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-120/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-120/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-40] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-100] due to args.save_total_limit
  8%|▊         | 123/1600 [00:32<13:51,  1.78it/s]  8%|▊         | 126/1600 [00:32<10:00,  2.45it/s]  8%|▊         | 129/1600 [00:32<07:16,  3.37it/s]  8%|▊         | 132/1600 [00:32<05:22,  4.55it/s]  8%|▊         | 135/1600 [00:33<04:03,  6.03it/s]  9%|▊         | 138/1600 [00:33<03:06,  7.85it/s]                                                    9%|▉         | 140/1600 [00:33<03:05,  7.85it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.04it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.45it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.81it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.42it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.37it/s][A                                                  
                                               [A  9%|▉         | 140/1600 [00:34<03:05,  7.85it/s]
100%|██████████| 20/20 [00:00<00:00, 26.37it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-140
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-140/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-140/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-120] due to args.save_total_limit
  9%|▉         | 141/1600 [00:37<13:33,  1.79it/s]  9%|▉         | 143/1600 [00:37<10:48,  2.25it/s]  9%|▉         | 146/1600 [00:38<07:37,  3.18it/s]  9%|▉         | 149/1600 [00:38<05:29,  4.41it/s] 10%|▉         | 152/1600 [00:38<04:05,  5.90it/s] 10%|▉         | 155/1600 [00:38<03:06,  7.75it/s] 10%|▉         | 158/1600 [00:38<02:26,  9.82it/s]                                                   10%|█         | 160/1600 [00:38<02:26,  9.82it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.83it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.44it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.38it/s][A                                                  
                                               [A 10%|█         | 160/1600 [00:39<02:26,  9.82it/s]
100%|██████████| 20/20 [00:00<00:00, 26.38it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-160
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-160/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-160/special_tokens_map.json
 10%|█         | 161/1600 [00:42<12:31,  1.91it/s] 10%|█         | 163/1600 [00:43<09:58,  2.40it/s] 10%|█         | 166/1600 [00:43<07:05,  3.37it/s] 11%|█         | 169/1600 [00:43<05:10,  4.60it/s] 11%|█         | 172/1600 [00:43<03:50,  6.20it/s] 11%|█         | 175/1600 [00:43<02:56,  8.08it/s] 11%|█         | 178/1600 [00:43<02:20, 10.14it/s]                                                   11%|█▏        | 180/1600 [00:43<02:20, 10.14it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.47it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.82it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.39it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.31it/s][A                                                  
                                               [A 11%|█▏        | 180/1600 [00:44<02:20, 10.14it/s]
100%|██████████| 20/20 [00:00<00:00, 26.31it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-180
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-180/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-180/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-160] due to args.save_total_limit
 11%|█▏        | 181/1600 [00:48<12:34,  1.88it/s] 11%|█▏        | 183/1600 [00:48<10:00,  2.36it/s] 12%|█▏        | 186/1600 [00:48<07:04,  3.33it/s] 12%|█▏        | 189/1600 [00:48<05:06,  4.60it/s] 12%|█▏        | 192/1600 [00:48<03:49,  6.13it/s] 12%|█▏        | 195/1600 [00:48<02:56,  7.97it/s] 12%|█▏        | 198/1600 [00:49<02:19, 10.04it/s]                                                   12%|█▎        | 200/1600 [00:49<02:19, 10.04it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.88it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.35it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.92it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.37it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.39it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.29it/s][A                                                  
                                               [A 12%|█▎        | 200/1600 [00:50<02:19, 10.04it/s]
100%|██████████| 20/20 [00:00<00:00, 27.29it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-200
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-200/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-140] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-180] due to args.save_total_limit
 13%|█▎        | 201/1600 [00:54<13:41,  1.70it/s] 13%|█▎        | 204/1600 [00:54<09:50,  2.37it/s] 13%|█▎        | 207/1600 [00:54<07:08,  3.25it/s] 13%|█▎        | 210/1600 [00:54<05:17,  4.38it/s] 13%|█▎        | 213/1600 [00:54<03:57,  5.83it/s] 14%|█▎        | 216/1600 [00:54<03:04,  7.50it/s] 14%|█▎        | 219/1600 [00:54<02:25,  9.50it/s]                                                   14%|█▍        | 220/1600 [00:54<02:25,  9.50it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.04it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.08it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.46it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.43it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A                                                  
                                               [A 14%|█▍        | 220/1600 [00:55<02:25,  9.50it/s]
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-220
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-220/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-220/special_tokens_map.json
 14%|█▍        | 222/1600 [00:59<11:52,  1.93it/s] 14%|█▍        | 224/1600 [00:59<09:28,  2.42it/s] 14%|█▍        | 227/1600 [00:59<06:42,  3.41it/s] 14%|█▍        | 230/1600 [00:59<04:52,  4.69it/s] 15%|█▍        | 233/1600 [00:59<03:39,  6.24it/s] 15%|█▍        | 236/1600 [00:59<02:47,  8.17it/s] 15%|█▍        | 239/1600 [01:00<02:12, 10.30it/s]                                                   15%|█▌        | 240/1600 [01:00<02:12, 10.30it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.47it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.82it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.43it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.38it/s][A                                                  
                                               [A 15%|█▌        | 240/1600 [01:00<02:12, 10.30it/s]
100%|██████████| 20/20 [00:00<00:00, 26.38it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-240
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-240/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-240/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-220] due to args.save_total_limit
 15%|█▌        | 242/1600 [01:04<11:58,  1.89it/s] 15%|█▌        | 245/1600 [01:04<08:37,  2.62it/s] 16%|█▌        | 248/1600 [01:04<06:16,  3.59it/s] 16%|█▌        | 251/1600 [01:04<04:39,  4.82it/s] 16%|█▌        | 254/1600 [01:05<03:32,  6.32it/s] 16%|█▌        | 257/1600 [01:05<02:45,  8.11it/s] 16%|█▋        | 260/1600 [01:05<02:09, 10.36it/s]                                                   16%|█▋        | 260/1600 [01:05<02:09, 10.36it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.04it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.81it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.43it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.37it/s][A                                                  
                                               [A 16%|█▋        | 260/1600 [01:06<02:09, 10.36it/s]
100%|██████████| 20/20 [00:00<00:00, 26.37it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-260
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-260/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-260/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-200] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-240] due to args.save_total_limit
 16%|█▋        | 263/1600 [01:10<12:32,  1.78it/s] 17%|█▋        | 266/1600 [01:10<09:03,  2.46it/s] 17%|█▋        | 269/1600 [01:10<06:36,  3.36it/s] 17%|█▋        | 272/1600 [01:10<04:52,  4.53it/s] 17%|█▋        | 275/1600 [01:10<03:42,  5.96it/s] 17%|█▋        | 278/1600 [01:10<02:51,  7.71it/s]                                                   18%|█▊        | 280/1600 [01:11<02:51,  7.71it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.08it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.48it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.84it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.45it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.32it/s][A                                                  
                                               [A 18%|█▊        | 280/1600 [01:11<02:51,  7.71it/s]
100%|██████████| 20/20 [00:00<00:00, 26.32it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-280
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-280/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-280/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-260] due to args.save_total_limit
 18%|█▊        | 281/1600 [01:15<12:08,  1.81it/s] 18%|█▊        | 284/1600 [01:15<08:47,  2.49it/s] 18%|█▊        | 287/1600 [01:15<06:25,  3.41it/s] 18%|█▊        | 290/1600 [01:15<04:45,  4.59it/s] 18%|█▊        | 293/1600 [01:16<03:36,  6.05it/s] 18%|█▊        | 296/1600 [01:16<02:47,  7.80it/s] 19%|█▊        | 299/1600 [01:16<02:11,  9.89it/s]                                                   19%|█▉        | 300/1600 [01:16<02:11,  9.89it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.05it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.48it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.83it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.44it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.39it/s][A                                                  
                                               [A 19%|█▉        | 300/1600 [01:17<02:11,  9.89it/s]
100%|██████████| 20/20 [00:00<00:00, 26.39it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-300
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-300/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-300/special_tokens_map.json
 19%|█▉        | 302/1600 [01:20<11:07,  1.94it/s] 19%|█▉        | 305/1600 [01:20<08:02,  2.68it/s] 19%|█▉        | 308/1600 [01:20<05:54,  3.64it/s] 19%|█▉        | 311/1600 [01:21<04:24,  4.88it/s] 20%|█▉        | 314/1600 [01:21<03:20,  6.42it/s] 20%|█▉        | 317/1600 [01:21<02:35,  8.23it/s] 20%|██        | 320/1600 [01:21<02:02, 10.49it/s]                                                   20%|██        | 320/1600 [01:21<02:02, 10.49it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.29it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.25it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.00it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.42it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.43it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.32it/s][A                                                  
                                               [A 20%|██        | 320/1600 [01:22<02:02, 10.49it/s]
100%|██████████| 20/20 [00:00<00:00, 27.32it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-320
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-320/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-320/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-300] due to args.save_total_limit
 20%|██        | 323/1600 [01:26<11:52,  1.79it/s] 20%|██        | 326/1600 [01:26<08:32,  2.48it/s] 21%|██        | 329/1600 [01:26<06:15,  3.39it/s] 21%|██        | 332/1600 [01:26<04:38,  4.55it/s] 21%|██        | 335/1600 [01:26<03:30,  6.01it/s] 21%|██        | 338/1600 [01:27<02:43,  7.70it/s]                                                   21%|██▏       | 340/1600 [01:27<02:43,  7.70it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.95it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.30it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.01it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.41it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.43it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.31it/s][A                                                  
                                               [A 21%|██▏       | 340/1600 [01:28<02:43,  7.70it/s]
100%|██████████| 20/20 [00:00<00:00, 27.31it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-340
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-340/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-340/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-280] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-320] due to args.save_total_limit
 21%|██▏       | 341/1600 [01:31<11:55,  1.76it/s] 22%|██▏       | 344/1600 [01:31<08:37,  2.42it/s] 22%|██▏       | 347/1600 [01:32<06:17,  3.32it/s] 22%|██▏       | 350/1600 [01:32<04:37,  4.50it/s] 22%|██▏       | 353/1600 [01:32<03:29,  5.96it/s] 22%|██▏       | 356/1600 [01:32<02:40,  7.77it/s] 22%|██▏       | 359/1600 [01:32<02:07,  9.73it/s]                                                   22%|██▎       | 360/1600 [01:32<02:07,  9.73it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.04it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.79it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.41it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.37it/s][A                                                  
                                               [A 22%|██▎       | 360/1600 [01:33<02:07,  9.73it/s]
100%|██████████| 20/20 [00:00<00:00, 26.37it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-360
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-360/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-360/special_tokens_map.json
 23%|██▎       | 362/1600 [01:36<10:42,  1.93it/s] 23%|██▎       | 365/1600 [01:37<07:45,  2.66it/s] 23%|██▎       | 368/1600 [01:37<05:40,  3.62it/s] 23%|██▎       | 371/1600 [01:37<04:13,  4.84it/s] 23%|██▎       | 374/1600 [01:37<03:12,  6.38it/s] 24%|██▎       | 377/1600 [01:37<02:31,  8.09it/s] 24%|██▍       | 380/1600 [01:37<01:58, 10.31it/s]                                                   24%|██▍       | 380/1600 [01:37<01:58, 10.31it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.07it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.49it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.83it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.42it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.33it/s][A                                                  
                                               [A 24%|██▍       | 380/1600 [01:38<01:58, 10.31it/s]
100%|██████████| 20/20 [00:00<00:00, 26.33it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-380
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-380/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-380/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-360] due to args.save_total_limit
 24%|██▍       | 383/1600 [01:42<10:37,  1.91it/s] 24%|██▍       | 386/1600 [01:42<07:41,  2.63it/s] 24%|██▍       | 389/1600 [01:42<05:38,  3.58it/s] 24%|██▍       | 392/1600 [01:42<04:11,  4.80it/s] 25%|██▍       | 395/1600 [01:42<03:11,  6.30it/s] 25%|██▍       | 398/1600 [01:42<02:28,  8.11it/s]                                                   25%|██▌       | 400/1600 [01:43<02:27,  8.11it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.79it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.42it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.36it/s][A                                                  
                                               [A 25%|██▌       | 400/1600 [01:43<02:27,  8.11it/s]
100%|██████████| 20/20 [00:00<00:00, 26.36it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-400
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-400/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-380] due to args.save_total_limit
 25%|██▌       | 401/1600 [01:47<11:01,  1.81it/s] 25%|██▌       | 404/1600 [01:47<07:57,  2.50it/s] 25%|██▌       | 407/1600 [01:47<05:48,  3.42it/s] 26%|██▌       | 410/1600 [01:48<04:19,  4.58it/s] 26%|██▌       | 413/1600 [01:48<03:16,  6.04it/s] 26%|██▌       | 416/1600 [01:48<02:30,  7.86it/s] 26%|██▌       | 419/1600 [01:48<01:59,  9.84it/s]                                                   26%|██▋       | 420/1600 [01:48<01:59,  9.84it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.97it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.29it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.98it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.40it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.42it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.30it/s][A                                                  
                                               [A 26%|██▋       | 420/1600 [01:49<01:59,  9.84it/s]
100%|██████████| 20/20 [00:00<00:00, 27.30it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-420
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-420/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-420/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-340] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-400] due to args.save_total_limit
 26%|██▋       | 422/1600 [01:53<10:40,  1.84it/s] 27%|██▋       | 425/1600 [01:53<07:42,  2.54it/s] 27%|██▋       | 428/1600 [01:53<05:37,  3.47it/s] 27%|██▋       | 431/1600 [01:53<04:10,  4.66it/s] 27%|██▋       | 434/1600 [01:53<03:08,  6.18it/s] 27%|██▋       | 437/1600 [01:53<02:26,  7.91it/s] 28%|██▊       | 440/1600 [01:53<01:54, 10.10it/s]                                                   28%|██▊       | 440/1600 [01:53<01:54, 10.10it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.08it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.79it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.41it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.36it/s][A                                                  
                                               [A 28%|██▊       | 440/1600 [01:54<01:54, 10.10it/s]
100%|██████████| 20/20 [00:00<00:00, 26.36it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-440
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-440/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-440/special_tokens_map.json
 28%|██▊       | 443/1600 [01:58<10:29,  1.84it/s] 28%|██▊       | 446/1600 [01:58<07:34,  2.54it/s] 28%|██▊       | 449/1600 [01:58<05:31,  3.47it/s] 28%|██▊       | 452/1600 [01:58<04:05,  4.67it/s] 28%|██▊       | 455/1600 [01:59<03:06,  6.15it/s] 29%|██▊       | 458/1600 [01:59<02:23,  7.97it/s]                                                   29%|██▉       | 460/1600 [01:59<02:23,  7.97it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.96it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.08it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.37it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.25it/s][A
100%|██████████| 20/20 [00:00<00:00, 26.99it/s][A                                                  
                                               [A 29%|██▉       | 460/1600 [02:00<02:23,  7.97it/s]
100%|██████████| 20/20 [00:00<00:00, 26.99it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-460
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-460/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-460/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-440] due to args.save_total_limit
 29%|██▉       | 461/1600 [02:03<10:36,  1.79it/s] 29%|██▉       | 464/1600 [02:04<07:40,  2.47it/s] 29%|██▉       | 467/1600 [02:04<05:35,  3.38it/s] 29%|██▉       | 470/1600 [02:04<04:08,  4.54it/s] 30%|██▉       | 473/1600 [02:04<03:07,  6.00it/s] 30%|██▉       | 476/1600 [02:04<02:23,  7.82it/s] 30%|██▉       | 479/1600 [02:04<01:55,  9.70it/s]                                                   30%|███       | 480/1600 [02:04<01:55,  9.70it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.72it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.31it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.00it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.36it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.36it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A                                                  
                                               [A 30%|███       | 480/1600 [02:05<01:55,  9.70it/s]
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-480
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-480/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-480/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-460] due to args.save_total_limit
 30%|███       | 482/1600 [02:09<10:05,  1.85it/s] 30%|███       | 485/1600 [02:09<07:17,  2.55it/s] 30%|███       | 488/1600 [02:09<05:21,  3.46it/s] 31%|███       | 491/1600 [02:09<03:57,  4.68it/s] 31%|███       | 494/1600 [02:09<02:58,  6.21it/s] 31%|███       | 497/1600 [02:10<02:19,  7.90it/s] 31%|███▏      | 500/1600 [02:10<01:49, 10.05it/s]                                                   31%|███▏      | 500/1600 [02:10<01:49, 10.05it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.98it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.30it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.96it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.38it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.40it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.29it/s][A                                                  
                                               [A 31%|███▏      | 500/1600 [02:11<01:49, 10.05it/s]
100%|██████████| 20/20 [00:00<00:00, 27.29it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-500
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-500/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-420] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-480] due to args.save_total_limit
 31%|███▏      | 503/1600 [02:14<10:01,  1.82it/s] 32%|███▏      | 506/1600 [02:15<07:13,  2.52it/s] 32%|███▏      | 509/1600 [02:15<05:16,  3.45it/s] 32%|███▏      | 512/1600 [02:15<03:53,  4.66it/s] 32%|███▏      | 515/1600 [02:15<02:56,  6.16it/s] 32%|███▏      | 518/1600 [02:15<02:15,  7.97it/s]                                                   32%|███▎      | 520/1600 [02:15<02:15,  7.97it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.04it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.78it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.37it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.33it/s][A                                                  
                                               [A 32%|███▎      | 520/1600 [02:16<02:15,  7.97it/s]
100%|██████████| 20/20 [00:00<00:00, 26.33it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-520
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-520/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-520/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-500] due to args.save_total_limit
 33%|███▎      | 521/1600 [02:20<09:55,  1.81it/s] 33%|███▎      | 524/1600 [02:20<07:10,  2.50it/s] 33%|███▎      | 527/1600 [02:20<05:14,  3.41it/s] 33%|███▎      | 530/1600 [02:20<03:52,  4.60it/s] 33%|███▎      | 533/1600 [02:20<02:54,  6.10it/s] 34%|███▎      | 536/1600 [02:20<02:15,  7.86it/s] 34%|███▎      | 539/1600 [02:20<01:46, 10.00it/s]                                                   34%|███▍      | 540/1600 [02:20<01:45, 10.00it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.70it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.26it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.97it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.39it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.40it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.29it/s][A                                                  
                                               [A 34%|███▍      | 540/1600 [02:21<01:45, 10.00it/s]
100%|██████████| 20/20 [00:00<00:00, 27.29it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-540
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-540/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-540/special_tokens_map.json
 34%|███▍      | 542/1600 [02:25<09:05,  1.94it/s] 34%|███▍      | 545/1600 [02:25<06:33,  2.68it/s] 34%|███▍      | 548/1600 [02:25<04:47,  3.66it/s] 34%|███▍      | 551/1600 [02:25<03:33,  4.92it/s] 35%|███▍      | 554/1600 [02:25<02:42,  6.42it/s] 35%|███▍      | 557/1600 [02:25<02:06,  8.26it/s] 35%|███▌      | 560/1600 [02:26<01:39, 10.50it/s]                                                   35%|███▌      | 560/1600 [02:26<01:39, 10.50it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.06it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.82it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.43it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.38it/s][A                                                  
                                               [A 35%|███▌      | 560/1600 [02:27<01:39, 10.50it/s]
100%|██████████| 20/20 [00:00<00:00, 26.38it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-560
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-560/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-560/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-520] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-540] due to args.save_total_limit
 35%|███▌      | 563/1600 [02:30<09:26,  1.83it/s] 35%|███▌      | 566/1600 [02:31<06:49,  2.52it/s] 36%|███▌      | 569/1600 [02:31<04:58,  3.45it/s] 36%|███▌      | 572/1600 [02:31<03:40,  4.67it/s] 36%|███▌      | 575/1600 [02:31<02:46,  6.16it/s] 36%|███▌      | 578/1600 [02:31<02:09,  7.92it/s]                                                   36%|███▋      | 580/1600 [02:31<02:08,  7.92it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.09it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.45it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.43it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.30it/s][A                                                  
                                               [A 36%|███▋      | 580/1600 [02:32<02:08,  7.92it/s]
100%|██████████| 20/20 [00:00<00:00, 27.30it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-580
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-580/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-580/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-560] due to args.save_total_limit
 36%|███▋      | 581/1600 [02:36<09:57,  1.71it/s] 36%|███▋      | 584/1600 [02:36<07:10,  2.36it/s] 37%|███▋      | 587/1600 [02:36<05:13,  3.23it/s] 37%|███▋      | 590/1600 [02:36<03:51,  4.36it/s] 37%|███▋      | 593/1600 [02:37<02:54,  5.75it/s] 37%|███▋      | 596/1600 [02:37<02:14,  7.46it/s] 37%|███▋      | 599/1600 [02:37<01:45,  9.48it/s]                                                   38%|███▊      | 600/1600 [02:37<01:45,  9.48it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.69it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.30it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.28it/s][A                                                  
                                               [A 38%|███▊      | 600/1600 [02:38<01:45,  9.48it/s]
100%|██████████| 20/20 [00:00<00:00, 26.28it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-600
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-600/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-600/special_tokens_map.json
 38%|███▊      | 602/1600 [02:41<08:50,  1.88it/s] 38%|███▊      | 604/1600 [02:41<07:02,  2.36it/s] 38%|███▊      | 607/1600 [02:42<05:00,  3.30it/s] 38%|███▊      | 610/1600 [02:42<03:39,  4.52it/s] 38%|███▊      | 613/1600 [02:42<02:42,  6.06it/s] 38%|███▊      | 616/1600 [02:42<02:05,  7.86it/s] 39%|███▊      | 619/1600 [02:42<01:39,  9.85it/s]                                                   39%|███▉      | 620/1600 [02:42<01:39,  9.85it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.83it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.39it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.06it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.45it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.43it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.30it/s][A                                                  
                                               [A 39%|███▉      | 620/1600 [02:43<01:39,  9.85it/s]
100%|██████████| 20/20 [00:00<00:00, 27.30it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-620
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-620/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-620/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-600] due to args.save_total_limit
 39%|███▉      | 622/1600 [02:47<08:41,  1.87it/s] 39%|███▉      | 625/1600 [02:47<06:15,  2.60it/s] 39%|███▉      | 628/1600 [02:47<04:32,  3.57it/s] 39%|███▉      | 631/1600 [02:47<03:22,  4.78it/s] 40%|███▉      | 634/1600 [02:47<02:32,  6.32it/s] 40%|███▉      | 637/1600 [02:47<01:58,  8.16it/s] 40%|████      | 640/1600 [02:47<01:32, 10.35it/s]                                                   40%|████      | 640/1600 [02:47<01:32, 10.35it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.06it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.80it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.41it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.36it/s][A                                                  
                                               [A 40%|████      | 640/1600 [02:48<01:32, 10.35it/s]
100%|██████████| 20/20 [00:00<00:00, 26.36it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-640
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-640/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-640/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-580] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-620] due to args.save_total_limit
 40%|████      | 643/1600 [02:52<08:53,  1.79it/s] 40%|████      | 645/1600 [02:52<07:05,  2.25it/s] 40%|████      | 648/1600 [02:52<04:58,  3.19it/s] 41%|████      | 651/1600 [02:53<03:36,  4.38it/s] 41%|████      | 654/1600 [02:53<02:41,  5.87it/s] 41%|████      | 657/1600 [02:53<02:03,  7.62it/s] 41%|████▏     | 660/1600 [02:53<01:36,  9.73it/s]                                                   41%|████▏     | 660/1600 [02:53<01:36,  9.73it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.06it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.81it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.42it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.36it/s][A                                                  
                                               [A 41%|████▏     | 660/1600 [02:54<01:36,  9.73it/s]
100%|██████████| 20/20 [00:00<00:00, 26.36it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-660
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-660/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-660/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-640] due to args.save_total_limit
 41%|████▏     | 663/1600 [02:58<08:28,  1.84it/s] 42%|████▏     | 666/1600 [02:58<06:06,  2.55it/s] 42%|████▏     | 669/1600 [02:58<04:27,  3.49it/s] 42%|████▏     | 672/1600 [02:58<03:17,  4.71it/s] 42%|████▏     | 675/1600 [02:58<02:29,  6.19it/s] 42%|████▏     | 678/1600 [02:58<01:56,  7.93it/s]                                                   42%|████▎     | 680/1600 [02:58<01:55,  7.93it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.93it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.09it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.46it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.44it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.32it/s][A                                                  
                                               [A 42%|████▎     | 680/1600 [02:59<01:55,  7.93it/s]
100%|██████████| 20/20 [00:00<00:00, 27.32it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-680
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-680/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-680/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-660] due to args.save_total_limit
 43%|████▎     | 681/1600 [03:03<08:31,  1.80it/s] 43%|████▎     | 684/1600 [03:03<06:10,  2.47it/s] 43%|████▎     | 687/1600 [03:03<04:29,  3.38it/s] 43%|████▎     | 690/1600 [03:03<03:19,  4.57it/s] 43%|████▎     | 693/1600 [03:03<02:30,  6.01it/s] 44%|████▎     | 696/1600 [03:04<01:56,  7.75it/s] 44%|████▎     | 699/1600 [03:04<01:32,  9.69it/s]                                                   44%|████▍     | 700/1600 [03:04<01:32,  9.69it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.02it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.76it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.35it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.31it/s][A                                                  
                                               [A 44%|████▍     | 700/1600 [03:05<01:32,  9.69it/s]
100%|██████████| 20/20 [00:00<00:00, 26.31it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-700
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-700/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-680] due to args.save_total_limit
 44%|████▍     | 702/1600 [03:08<08:02,  1.86it/s] 44%|████▍     | 705/1600 [03:09<05:48,  2.57it/s] 44%|████▍     | 708/1600 [03:09<04:14,  3.51it/s] 44%|████▍     | 711/1600 [03:09<03:09,  4.69it/s] 45%|████▍     | 714/1600 [03:09<02:22,  6.24it/s] 45%|████▍     | 717/1600 [03:09<01:50,  8.00it/s] 45%|████▌     | 720/1600 [03:09<01:26, 10.23it/s]                                                   45%|████▌     | 720/1600 [03:09<01:26, 10.23it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.00it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.06it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.40it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.40it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A                                                  
                                               [A 45%|████▌     | 720/1600 [03:10<01:26, 10.23it/s]
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-720
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-720/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-720/special_tokens_map.json
 45%|████▌     | 723/1600 [03:14<07:33,  1.94it/s] 45%|████▌     | 726/1600 [03:14<05:27,  2.67it/s] 46%|████▌     | 729/1600 [03:14<03:59,  3.64it/s] 46%|████▌     | 732/1600 [03:14<02:58,  4.88it/s] 46%|████▌     | 735/1600 [03:14<02:14,  6.43it/s] 46%|████▌     | 738/1600 [03:14<01:44,  8.21it/s]                                                   46%|████▋     | 740/1600 [03:14<01:44,  8.21it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.02it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.79it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.40it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.32it/s][A                                                  
                                               [A 46%|████▋     | 740/1600 [03:15<01:44,  8.21it/s]
100%|██████████| 20/20 [00:00<00:00, 26.32it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-740
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-740/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-740/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-720] due to args.save_total_limit
 46%|████▋     | 741/1600 [03:19<07:41,  1.86it/s] 46%|████▋     | 744/1600 [03:19<05:34,  2.56it/s] 47%|████▋     | 747/1600 [03:19<04:04,  3.48it/s] 47%|████▋     | 750/1600 [03:19<03:01,  4.67it/s] 47%|████▋     | 753/1600 [03:19<02:16,  6.19it/s] 47%|████▋     | 756/1600 [03:19<01:46,  7.93it/s] 47%|████▋     | 759/1600 [03:20<01:24, 10.01it/s]                                                   48%|████▊     | 760/1600 [03:20<01:23, 10.01it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.81it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.34it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.01it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.39it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.36it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.21it/s][A                                                  
                                               [A 48%|████▊     | 760/1600 [03:21<01:23, 10.01it/s]
100%|██████████| 20/20 [00:00<00:00, 27.21it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-760
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-760/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-760/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-740] due to args.save_total_limit
 48%|████▊     | 762/1600 [03:24<07:26,  1.88it/s] 48%|████▊     | 765/1600 [03:24<05:22,  2.59it/s] 48%|████▊     | 768/1600 [03:24<03:56,  3.52it/s] 48%|████▊     | 771/1600 [03:25<02:54,  4.76it/s] 48%|████▊     | 774/1600 [03:25<02:12,  6.24it/s] 49%|████▊     | 777/1600 [03:25<01:43,  7.99it/s] 49%|████▉     | 780/1600 [03:25<01:21, 10.06it/s]                                                   49%|████▉     | 780/1600 [03:25<01:21, 10.06it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.06it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.67it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.31it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.28it/s][A                                                  
                                               [A 49%|████▉     | 780/1600 [03:26<01:21, 10.06it/s]
100%|██████████| 20/20 [00:00<00:00, 26.28it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-780
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-780/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-780/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-760] due to args.save_total_limit
 49%|████▉     | 783/1600 [03:29<07:09,  1.90it/s] 49%|████▉     | 786/1600 [03:30<05:10,  2.62it/s] 49%|████▉     | 789/1600 [03:30<03:46,  3.58it/s] 50%|████▉     | 792/1600 [03:30<02:49,  4.78it/s] 50%|████▉     | 795/1600 [03:30<02:08,  6.27it/s] 50%|████▉     | 798/1600 [03:30<01:39,  8.08it/s]                                                   50%|█████     | 800/1600 [03:30<01:38,  8.08it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.96it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.40it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.93it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.31it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.32it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.18it/s][A                                                  
                                               [A 50%|█████     | 800/1600 [03:31<01:38,  8.08it/s]
100%|██████████| 20/20 [00:00<00:00, 27.18it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-800
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-800/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-700] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-780] due to args.save_total_limit
 50%|█████     | 801/1600 [03:35<07:34,  1.76it/s] 50%|█████     | 803/1600 [03:35<06:02,  2.20it/s] 50%|█████     | 806/1600 [03:35<04:15,  3.11it/s] 51%|█████     | 809/1600 [03:35<03:05,  4.27it/s] 51%|█████     | 812/1600 [03:35<02:16,  5.75it/s] 51%|█████     | 815/1600 [03:36<01:44,  7.53it/s] 51%|█████     | 818/1600 [03:36<01:22,  9.52it/s]                                                   51%|█████▏    | 820/1600 [03:36<01:21,  9.52it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.71it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.29it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.26it/s][A                                                  
                                               [A 51%|█████▏    | 820/1600 [03:37<01:21,  9.52it/s]
100%|██████████| 20/20 [00:00<00:00, 26.26it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-820
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-820/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-820/special_tokens_map.json
 51%|█████▏    | 821/1600 [03:40<06:58,  1.86it/s] 52%|█████▏    | 824/1600 [03:40<05:01,  2.57it/s] 52%|█████▏    | 827/1600 [03:41<03:40,  3.51it/s] 52%|█████▏    | 830/1600 [03:41<02:43,  4.70it/s] 52%|█████▏    | 833/1600 [03:41<02:02,  6.26it/s] 52%|█████▏    | 836/1600 [03:41<01:34,  8.06it/s] 52%|█████▏    | 839/1600 [03:41<01:15, 10.09it/s]                                                   52%|█████▎    | 840/1600 [03:41<01:15, 10.09it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.08it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.48it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.81it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.42it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.35it/s][A                                                  
                                               [A 52%|█████▎    | 840/1600 [03:42<01:15, 10.09it/s]
100%|██████████| 20/20 [00:00<00:00, 26.35it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-840
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-840/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-840/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-820] due to args.save_total_limit
 53%|█████▎    | 842/1600 [03:46<06:59,  1.81it/s] 53%|█████▎    | 845/1600 [03:46<05:02,  2.49it/s] 53%|█████▎    | 848/1600 [03:46<03:40,  3.41it/s] 53%|█████▎    | 851/1600 [03:46<02:43,  4.57it/s] 53%|█████▎    | 854/1600 [03:46<02:04,  6.01it/s] 54%|█████▎    | 857/1600 [03:47<01:35,  7.78it/s] 54%|█████▍    | 860/1600 [03:47<01:14,  9.93it/s]                                                   54%|█████▍    | 860/1600 [03:47<01:14,  9.93it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.95it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.39it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.05it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.43it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.42it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A                                                  
                                               [A 54%|█████▍    | 860/1600 [03:48<01:14,  9.93it/s]
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-860
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-860/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-860/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-800] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-840] due to args.save_total_limit
 54%|█████▍    | 863/1600 [03:51<06:44,  1.82it/s] 54%|█████▍    | 865/1600 [03:52<05:22,  2.28it/s] 54%|█████▍    | 868/1600 [03:52<03:47,  3.22it/s] 54%|█████▍    | 871/1600 [03:52<02:45,  4.42it/s] 55%|█████▍    | 874/1600 [03:52<02:03,  5.89it/s] 55%|█████▍    | 877/1600 [03:52<01:34,  7.66it/s] 55%|█████▌    | 880/1600 [03:52<01:13,  9.85it/s]                                                   55%|█████▌    | 880/1600 [03:52<01:13,  9.85it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.02it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.45it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.81it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.29it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.16it/s][A                                                  
                                               [A 55%|█████▌    | 880/1600 [03:53<01:13,  9.85it/s]
100%|██████████| 20/20 [00:00<00:00, 26.16it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-880
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-880/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-880/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-860] due to args.save_total_limit
 55%|█████▌    | 883/1600 [03:57<06:33,  1.82it/s] 55%|█████▌    | 885/1600 [03:57<05:12,  2.29it/s] 56%|█████▌    | 888/1600 [03:57<03:40,  3.24it/s] 56%|█████▌    | 891/1600 [03:57<02:40,  4.43it/s] 56%|█████▌    | 894/1600 [03:57<01:58,  5.94it/s] 56%|█████▌    | 897/1600 [03:57<01:30,  7.76it/s]                                                   56%|█████▋    | 900/1600 [03:58<01:30,  7.76it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.89it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.39it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.06it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.44it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.42it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.28it/s][A                                                  
                                               [A 56%|█████▋    | 900/1600 [03:59<01:30,  7.76it/s]
100%|██████████| 20/20 [00:00<00:00, 27.28it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-900/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-900/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-880] due to args.save_total_limit
 56%|█████▋    | 901/1600 [04:02<05:54,  1.97it/s] 56%|█████▋    | 904/1600 [04:02<04:21,  2.66it/s] 57%|█████▋    | 907/1600 [04:02<03:13,  3.57it/s] 57%|█████▋    | 910/1600 [04:02<02:24,  4.77it/s] 57%|█████▋    | 913/1600 [04:03<01:50,  6.19it/s] 57%|█████▋    | 916/1600 [04:03<01:26,  7.92it/s] 57%|█████▋    | 919/1600 [04:03<01:08, 10.00it/s]                                                   57%|█████▊    | 920/1600 [04:03<01:08, 10.00it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.08it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.45it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.44it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A                                                  
                                               [A 57%|█████▊    | 920/1600 [04:04<01:08, 10.00it/s]
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-920
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-920/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-920/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-900] due to args.save_total_limit
 58%|█████▊    | 922/1600 [04:08<06:07,  1.85it/s] 58%|█████▊    | 925/1600 [04:08<04:25,  2.54it/s] 58%|█████▊    | 928/1600 [04:08<03:13,  3.47it/s] 58%|█████▊    | 931/1600 [04:08<02:24,  4.63it/s] 58%|█████▊    | 934/1600 [04:08<01:49,  6.09it/s] 59%|█████▊    | 937/1600 [04:08<01:24,  7.81it/s] 59%|█████▉    | 940/1600 [04:08<01:06,  9.92it/s]                                                   59%|█████▉    | 940/1600 [04:08<01:06,  9.92it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.24it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.20it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.93it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.36it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.37it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.20it/s][A                                                  
                                               [A 59%|█████▉    | 940/1600 [04:09<01:06,  9.92it/s]
100%|██████████| 20/20 [00:00<00:00, 27.20it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-940
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-940/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-940/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-920] due to args.save_total_limit
 59%|█████▉    | 943/1600 [04:13<05:58,  1.83it/s] 59%|█████▉    | 946/1600 [04:13<04:18,  2.53it/s] 59%|█████▉    | 949/1600 [04:13<03:08,  3.46it/s] 60%|█████▉    | 952/1600 [04:14<02:19,  4.65it/s] 60%|█████▉    | 955/1600 [04:14<01:45,  6.11it/s] 60%|█████▉    | 958/1600 [04:14<01:22,  7.79it/s]                                                   60%|██████    | 960/1600 [04:14<01:22,  7.79it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.96it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.39it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.04it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.40it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.30it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.12it/s][A                                                  
                                               [A 60%|██████    | 960/1600 [04:15<01:22,  7.79it/s]
100%|██████████| 20/20 [00:00<00:00, 27.12it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-960
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-960/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-960/special_tokens_map.json
 60%|██████    | 961/1600 [04:18<05:47,  1.84it/s] 60%|██████    | 963/1600 [04:18<04:37,  2.30it/s] 60%|██████    | 966/1600 [04:19<03:15,  3.25it/s] 61%|██████    | 969/1600 [04:19<02:21,  4.47it/s] 61%|██████    | 972/1600 [04:19<01:46,  5.92it/s] 61%|██████    | 975/1600 [04:19<01:21,  7.65it/s] 61%|██████    | 978/1600 [04:19<01:04,  9.69it/s]                                                   61%|██████▏   | 980/1600 [04:19<01:03,  9.69it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.96it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.42it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.93it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.30it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.33it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.17it/s][A                                                  
                                               [A 61%|██████▏   | 980/1600 [04:20<01:03,  9.69it/s]
100%|██████████| 20/20 [00:00<00:00, 27.17it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-980
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-980/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-980/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-940] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-960] due to args.save_total_limit
 61%|██████▏   | 981/1600 [04:24<05:43,  1.80it/s] 61%|██████▏   | 983/1600 [04:24<04:32,  2.26it/s] 62%|██████▏   | 986/1600 [04:24<03:11,  3.20it/s] 62%|██████▏   | 989/1600 [04:24<02:18,  4.40it/s] 62%|██████▏   | 992/1600 [04:24<01:42,  5.92it/s] 62%|██████▏   | 995/1600 [04:24<01:19,  7.65it/s] 62%|██████▏   | 998/1600 [04:25<01:02,  9.67it/s]                                                   62%|██████▎   | 1000/1600 [04:25<01:02,  9.67it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.42it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.02it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.42it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.37it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A                                                   
                                               [A 62%|██████▎   | 1000/1600 [04:26<01:02,  9.67it/s]
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1000
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1000/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1000/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-980] due to args.save_total_limit
 63%|██████▎   | 1001/1600 [04:29<05:26,  1.84it/s] 63%|██████▎   | 1004/1600 [04:29<03:55,  2.53it/s] 63%|██████▎   | 1007/1600 [04:30<02:50,  3.48it/s] 63%|██████▎   | 1010/1600 [04:30<02:05,  4.69it/s] 63%|██████▎   | 1013/1600 [04:30<01:34,  6.18it/s] 64%|██████▎   | 1016/1600 [04:30<01:13,  7.94it/s] 64%|██████▎   | 1019/1600 [04:30<00:58,  9.96it/s]                                                    64%|██████▍   | 1020/1600 [04:30<00:58,  9.96it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.02it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.42it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.90it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.28it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.16it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.04it/s][A                                                   
                                               [A 64%|██████▍   | 1020/1600 [04:31<00:58,  9.96it/s]
100%|██████████| 20/20 [00:00<00:00, 27.04it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1020
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1020/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1020/special_tokens_map.json
 64%|██████▍   | 1022/1600 [04:35<05:09,  1.86it/s] 64%|██████▍   | 1024/1600 [04:35<04:06,  2.34it/s] 64%|██████▍   | 1027/1600 [04:35<02:54,  3.29it/s] 64%|██████▍   | 1030/1600 [04:35<02:06,  4.51it/s] 65%|██████▍   | 1033/1600 [04:35<01:34,  6.03it/s] 65%|██████▍   | 1036/1600 [04:35<01:11,  7.85it/s] 65%|██████▍   | 1039/1600 [04:35<00:57,  9.83it/s]                                                    65%|██████▌   | 1040/1600 [04:35<00:56,  9.83it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.69it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.21it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.90it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.34it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.37it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.20it/s][A                                                   
                                               [A 65%|██████▌   | 1040/1600 [04:36<00:56,  9.83it/s]
100%|██████████| 20/20 [00:00<00:00, 27.20it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1040
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1040/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1040/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1000] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1020] due to args.save_total_limit
 65%|██████▌   | 1042/1600 [04:40<05:14,  1.77it/s] 65%|██████▌   | 1045/1600 [04:40<03:45,  2.46it/s] 66%|██████▌   | 1048/1600 [04:41<02:43,  3.37it/s] 66%|██████▌   | 1051/1600 [04:41<02:00,  4.54it/s] 66%|██████▌   | 1054/1600 [04:41<01:31,  5.97it/s] 66%|██████▌   | 1057/1600 [04:41<01:09,  7.78it/s] 66%|██████▋   | 1060/1600 [04:41<00:54,  9.94it/s]                                                    66%|██████▋   | 1060/1600 [04:41<00:54,  9.94it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.95it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.42it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.07it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.45it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.34it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.17it/s][A                                                   
                                               [A 66%|██████▋   | 1060/1600 [04:42<00:54,  9.94it/s]
100%|██████████| 20/20 [00:00<00:00, 27.17it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1060
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1060/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1060/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1040] due to args.save_total_limit
 66%|██████▋   | 1063/1600 [04:46<04:48,  1.86it/s] 67%|██████▋   | 1066/1600 [04:46<03:28,  2.56it/s] 67%|██████▋   | 1069/1600 [04:46<02:31,  3.50it/s] 67%|██████▋   | 1072/1600 [04:46<01:52,  4.68it/s] 67%|██████▋   | 1075/1600 [04:46<01:24,  6.21it/s] 67%|██████▋   | 1078/1600 [04:46<01:05,  7.99it/s]                                                    68%|██████▊   | 1080/1600 [04:46<01:05,  7.99it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.45it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.79it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.25it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.19it/s][A                                                   
                                               [A 68%|██████▊   | 1080/1600 [04:47<01:05,  7.99it/s]
100%|██████████| 20/20 [00:00<00:00, 26.19it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1080
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1080/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1080/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1060] due to args.save_total_limit
 68%|██████▊   | 1081/1600 [04:51<04:50,  1.79it/s] 68%|██████▊   | 1084/1600 [04:51<03:29,  2.47it/s] 68%|██████▊   | 1087/1600 [04:51<02:32,  3.36it/s] 68%|██████▊   | 1090/1600 [04:51<01:52,  4.52it/s] 68%|██████▊   | 1093/1600 [04:52<01:24,  5.97it/s] 68%|██████▊   | 1096/1600 [04:52<01:05,  7.70it/s] 69%|██████▊   | 1099/1600 [04:52<00:51,  9.76it/s]                                                    69%|██████▉   | 1100/1600 [04:52<00:51,  9.76it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.74it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.26it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.98it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.39it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.40it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A                                                   
                                               [A 69%|██████▉   | 1100/1600 [04:53<00:51,  9.76it/s]
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1100
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1100/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1100/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1080] due to args.save_total_limit
 69%|██████▉   | 1102/1600 [04:56<04:27,  1.86it/s] 69%|██████▉   | 1105/1600 [04:57<03:12,  2.57it/s] 69%|██████▉   | 1108/1600 [04:57<02:20,  3.49it/s] 69%|██████▉   | 1111/1600 [04:57<01:43,  4.71it/s] 70%|██████▉   | 1114/1600 [04:57<01:18,  6.21it/s] 70%|██████▉   | 1117/1600 [04:57<01:00,  7.97it/s] 70%|███████   | 1120/1600 [04:57<00:47, 10.15it/s]                                                    70%|███████   | 1120/1600 [04:57<00:47, 10.15it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.97it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.06it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.44it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.32it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.19it/s][A                                                   
                                               [A 70%|███████   | 1120/1600 [04:58<00:47, 10.15it/s]
100%|██████████| 20/20 [00:00<00:00, 27.19it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1120
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1120/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1120/special_tokens_map.json
 70%|███████   | 1123/1600 [05:02<04:07,  1.92it/s] 70%|███████   | 1126/1600 [05:02<02:58,  2.66it/s] 71%|███████   | 1129/1600 [05:02<02:10,  3.62it/s] 71%|███████   | 1132/1600 [05:02<01:36,  4.85it/s] 71%|███████   | 1135/1600 [05:02<01:13,  6.35it/s] 71%|███████   | 1138/1600 [05:02<00:56,  8.20it/s]                                                    71%|███████▏  | 1140/1600 [05:02<00:56,  8.20it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.06it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.47it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.80it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.36it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.24it/s][A                                                   
                                               [A 71%|███████▏  | 1140/1600 [05:03<00:56,  8.20it/s]
100%|██████████| 20/20 [00:00<00:00, 26.24it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1140
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1140/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1140/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1120] due to args.save_total_limit
 71%|███████▏  | 1141/1600 [05:07<04:06,  1.87it/s] 72%|███████▏  | 1144/1600 [05:07<02:57,  2.57it/s] 72%|███████▏  | 1147/1600 [05:07<02:08,  3.51it/s] 72%|███████▏  | 1150/1600 [05:07<01:35,  4.71it/s] 72%|███████▏  | 1153/1600 [05:07<01:11,  6.22it/s] 72%|███████▏  | 1156/1600 [05:08<00:55,  8.01it/s] 72%|███████▏  | 1159/1600 [05:08<00:44,  9.97it/s]                                                    72%|███████▎  | 1160/1600 [05:08<00:44,  9.97it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.08it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.42it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.29it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.15it/s][A                                                   
                                               [A 72%|███████▎  | 1160/1600 [05:09<00:44,  9.97it/s]
100%|██████████| 20/20 [00:00<00:00, 27.15it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1160
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1160/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1160/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1140] due to args.save_total_limit
 73%|███████▎  | 1162/1600 [05:12<03:59,  1.83it/s] 73%|███████▎  | 1165/1600 [05:13<02:52,  2.52it/s] 73%|███████▎  | 1168/1600 [05:13<02:05,  3.45it/s] 73%|███████▎  | 1171/1600 [05:13<01:32,  4.63it/s] 73%|███████▎  | 1174/1600 [05:13<01:09,  6.09it/s] 74%|███████▎  | 1177/1600 [05:13<00:53,  7.87it/s] 74%|███████▍  | 1180/1600 [05:13<00:42,  9.97it/s]                                                    74%|███████▍  | 1180/1600 [05:13<00:42,  9.97it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.00it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.42it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.07it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.45it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.34it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.19it/s][A                                                   
                                               [A 74%|███████▍  | 1180/1600 [05:14<00:42,  9.97it/s]
100%|██████████| 20/20 [00:00<00:00, 27.19it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1180
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1180/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1180/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1160] due to args.save_total_limit
 74%|███████▍  | 1183/1600 [05:18<03:39,  1.90it/s] 74%|███████▍  | 1186/1600 [05:18<02:38,  2.62it/s] 74%|███████▍  | 1189/1600 [05:18<01:54,  3.58it/s] 74%|███████▍  | 1192/1600 [05:18<01:25,  4.77it/s] 75%|███████▍  | 1195/1600 [05:18<01:04,  6.28it/s] 75%|███████▍  | 1198/1600 [05:18<00:49,  8.05it/s]                                                    75%|███████▌  | 1200/1600 [05:18<00:49,  8.05it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.04it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.46it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.81it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.36it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.24it/s][A                                                   
                                               [A 75%|███████▌  | 1200/1600 [05:19<00:49,  8.05it/s]
100%|██████████| 20/20 [00:00<00:00, 26.24it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1200
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1200/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1200/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1100] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1180] due to args.save_total_limit
 75%|███████▌  | 1201/1600 [05:23<03:48,  1.75it/s] 75%|███████▌  | 1204/1600 [05:23<02:43,  2.42it/s] 75%|███████▌  | 1207/1600 [05:23<01:58,  3.30it/s] 76%|███████▌  | 1210/1600 [05:24<01:27,  4.45it/s] 76%|███████▌  | 1213/1600 [05:24<01:06,  5.86it/s] 76%|███████▌  | 1216/1600 [05:24<00:50,  7.59it/s] 76%|███████▌  | 1219/1600 [05:24<00:40,  9.47it/s]                                                    76%|███████▋  | 1220/1600 [05:24<00:40,  9.47it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.07it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.45it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.43it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.27it/s][A                                                   
                                               [A 76%|███████▋  | 1220/1600 [05:25<00:40,  9.47it/s]
100%|██████████| 20/20 [00:00<00:00, 27.27it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1220
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1220/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1220/special_tokens_map.json
 76%|███████▋  | 1222/1600 [05:29<03:21,  1.87it/s] 77%|███████▋  | 1225/1600 [05:29<02:25,  2.58it/s] 77%|███████▋  | 1228/1600 [05:29<01:45,  3.53it/s] 77%|███████▋  | 1231/1600 [05:29<01:17,  4.74it/s] 77%|███████▋  | 1234/1600 [05:29<00:58,  6.21it/s] 77%|███████▋  | 1237/1600 [05:29<00:44,  8.11it/s]                                                    78%|███████▊  | 1240/1600 [05:29<00:44,  8.11it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.92it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.39it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.93it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.31it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.34it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A                                                   
                                               [A 78%|███████▊  | 1240/1600 [05:30<00:44,  8.11it/s]
100%|██████████| 20/20 [00:00<00:00, 27.23it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1240
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1240/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1240/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1200] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1220] due to args.save_total_limit
 78%|███████▊  | 1241/1600 [05:34<03:06,  1.92it/s] 78%|███████▊  | 1244/1600 [05:34<02:18,  2.58it/s] 78%|███████▊  | 1247/1600 [05:34<01:42,  3.46it/s] 78%|███████▊  | 1250/1600 [05:35<01:16,  4.59it/s] 78%|███████▊  | 1253/1600 [05:35<00:57,  6.02it/s] 78%|███████▊  | 1256/1600 [05:35<00:44,  7.71it/s] 79%|███████▊  | 1259/1600 [05:35<00:35,  9.67it/s]                                                    79%|███████▉  | 1260/1600 [05:35<00:35,  9.67it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.03it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.42it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.06it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.43it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.43it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.27it/s][A                                                   
                                               [A 79%|███████▉  | 1260/1600 [05:36<00:35,  9.67it/s]
100%|██████████| 20/20 [00:00<00:00, 27.27it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1260
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1260/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1260/special_tokens_map.json
 79%|███████▉  | 1262/1600 [05:39<02:54,  1.94it/s] 79%|███████▉  | 1265/1600 [05:39<02:05,  2.66it/s] 79%|███████▉  | 1268/1600 [05:40<01:31,  3.61it/s] 79%|███████▉  | 1271/1600 [05:40<01:07,  4.84it/s] 80%|███████▉  | 1274/1600 [05:40<00:51,  6.35it/s] 80%|███████▉  | 1277/1600 [05:40<00:39,  8.15it/s] 80%|████████  | 1280/1600 [05:40<00:30, 10.36it/s]                                                    80%|████████  | 1280/1600 [05:40<00:30, 10.36it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.84it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.39it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.05it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.44it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.29it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.15it/s][A                                                   
                                               [A 80%|████████  | 1280/1600 [05:41<00:30, 10.36it/s]
100%|██████████| 20/20 [00:00<00:00, 27.15it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1280
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1280/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1280/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1240] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1260] due to args.save_total_limit
 80%|████████  | 1283/1600 [05:45<02:56,  1.79it/s] 80%|████████  | 1286/1600 [05:45<02:07,  2.47it/s] 81%|████████  | 1289/1600 [05:45<01:32,  3.37it/s] 81%|████████  | 1292/1600 [05:45<01:07,  4.53it/s] 81%|████████  | 1295/1600 [05:46<00:50,  6.01it/s] 81%|████████  | 1298/1600 [05:46<00:39,  7.70it/s]                                                    81%|████████▏ | 1300/1600 [05:46<00:38,  7.70it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.93it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.41it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.06it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.44it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.44it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.28it/s][A                                                   
                                               [A 81%|████████▏ | 1300/1600 [05:47<00:38,  7.70it/s]
100%|██████████| 20/20 [00:00<00:00, 27.28it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1300
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1300/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1300/special_tokens_map.json
 81%|████████▏ | 1301/1600 [05:50<02:41,  1.85it/s] 82%|████████▏ | 1304/1600 [05:50<01:56,  2.55it/s] 82%|████████▏ | 1307/1600 [05:50<01:24,  3.48it/s] 82%|████████▏ | 1310/1600 [05:51<01:01,  4.68it/s] 82%|████████▏ | 1313/1600 [05:51<00:46,  6.19it/s] 82%|████████▏ | 1316/1600 [05:51<00:35,  7.93it/s] 82%|████████▏ | 1319/1600 [05:51<00:28,  9.86it/s]                                                    82%|████████▎ | 1320/1600 [05:51<00:28,  9.86it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 33.94it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.02it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.68it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.17it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.27it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.16it/s][A                                                   
                                               [A 82%|████████▎ | 1320/1600 [05:52<00:28,  9.86it/s]
100%|██████████| 20/20 [00:00<00:00, 27.16it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1320
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1320/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1320/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1300] due to args.save_total_limit
 83%|████████▎ | 1322/1600 [05:56<02:28,  1.87it/s] 83%|████████▎ | 1325/1600 [05:56<01:46,  2.57it/s] 83%|████████▎ | 1328/1600 [05:56<01:17,  3.50it/s] 83%|████████▎ | 1331/1600 [05:56<00:57,  4.71it/s] 83%|████████▎ | 1334/1600 [05:56<00:42,  6.22it/s] 84%|████████▎ | 1337/1600 [05:56<00:32,  7.99it/s] 84%|████████▍ | 1340/1600 [05:56<00:25, 10.19it/s]                                                    84%|████████▍ | 1340/1600 [05:56<00:25, 10.19it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.04it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.37it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.89it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.30it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.35it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A                                                   
                                               [A 84%|████████▍ | 1340/1600 [05:57<00:25, 10.19it/s]
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1340
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1340/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1340/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1280] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1320] due to args.save_total_limit
 84%|████████▍ | 1343/1600 [06:01<02:21,  1.82it/s] 84%|████████▍ | 1346/1600 [06:01<01:41,  2.51it/s] 84%|████████▍ | 1349/1600 [06:01<01:13,  3.41it/s] 84%|████████▍ | 1352/1600 [06:02<00:53,  4.62it/s] 85%|████████▍ | 1355/1600 [06:02<00:40,  6.11it/s] 85%|████████▍ | 1358/1600 [06:02<00:30,  7.90it/s]                                                    85%|████████▌ | 1360/1600 [06:02<00:30,  7.90it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.90it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.28it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.97it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.33it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.28it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.14it/s][A                                                   
                                               [A 85%|████████▌ | 1360/1600 [06:03<00:30,  7.90it/s]
100%|██████████| 20/20 [00:00<00:00, 27.14it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1360
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1360/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1360/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1340] due to args.save_total_limit
 85%|████████▌ | 1361/1600 [06:06<02:13,  1.79it/s] 85%|████████▌ | 1364/1600 [06:07<01:35,  2.47it/s] 85%|████████▌ | 1367/1600 [06:07<01:08,  3.38it/s] 86%|████████▌ | 1370/1600 [06:07<00:50,  4.55it/s] 86%|████████▌ | 1373/1600 [06:07<00:37,  6.00it/s] 86%|████████▌ | 1376/1600 [06:07<00:28,  7.73it/s] 86%|████████▌ | 1379/1600 [06:07<00:22,  9.67it/s]                                                    86%|████████▋ | 1380/1600 [06:07<00:22,  9.67it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.05it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.47it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.81it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.42it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.36it/s][A                                                   
                                               [A 86%|████████▋ | 1380/1600 [06:08<00:22,  9.67it/s]
100%|██████████| 20/20 [00:00<00:00, 26.36it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1380
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1380/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1380/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1360] due to args.save_total_limit
 86%|████████▋ | 1382/1600 [06:12<01:57,  1.85it/s] 87%|████████▋ | 1385/1600 [06:12<01:24,  2.55it/s] 87%|████████▋ | 1388/1600 [06:12<01:00,  3.49it/s] 87%|████████▋ | 1391/1600 [06:12<00:44,  4.69it/s] 87%|████████▋ | 1394/1600 [06:12<00:33,  6.21it/s] 87%|████████▋ | 1397/1600 [06:13<00:25,  7.98it/s] 88%|████████▊ | 1400/1600 [06:13<00:19, 10.15it/s]                                                    88%|████████▊ | 1400/1600 [06:13<00:19, 10.15it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.95it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.38it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.03it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.42it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.40it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A                                                   
                                               [A 88%|████████▊ | 1400/1600 [06:14<00:19, 10.15it/s]
100%|██████████| 20/20 [00:00<00:00, 27.26it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1400
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1400/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1400/special_tokens_map.json
 88%|████████▊ | 1403/1600 [06:17<01:43,  1.90it/s] 88%|████████▊ | 1406/1600 [06:17<01:14,  2.62it/s] 88%|████████▊ | 1409/1600 [06:17<00:53,  3.58it/s] 88%|████████▊ | 1412/1600 [06:18<00:39,  4.79it/s] 88%|████████▊ | 1415/1600 [06:18<00:29,  6.28it/s] 89%|████████▊ | 1418/1600 [06:18<00:22,  8.10it/s]                                                    89%|████████▉ | 1420/1600 [06:18<00:22,  8.10it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.94it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.96it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.33it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.26it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.16it/s][A                                                   
                                               [A 89%|████████▉ | 1420/1600 [06:19<00:22,  8.10it/s]
100%|██████████| 20/20 [00:00<00:00, 27.16it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1420
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1420/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1420/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1400] due to args.save_total_limit
 89%|████████▉ | 1421/1600 [06:22<01:37,  1.84it/s] 89%|████████▉ | 1424/1600 [06:23<01:09,  2.53it/s] 89%|████████▉ | 1427/1600 [06:23<00:50,  3.46it/s] 89%|████████▉ | 1430/1600 [06:23<00:36,  4.66it/s] 90%|████████▉ | 1433/1600 [06:23<00:27,  6.10it/s] 90%|████████▉ | 1436/1600 [06:23<00:20,  7.85it/s] 90%|████████▉ | 1439/1600 [06:23<00:16,  9.93it/s]                                                    90%|█████████ | 1440/1600 [06:23<00:16,  9.93it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.63it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.32it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.00it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.40it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.30it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.18it/s][A                                                   
                                               [A 90%|█████████ | 1440/1600 [06:24<00:16,  9.93it/s]
100%|██████████| 20/20 [00:00<00:00, 27.18it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1440
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1440/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1440/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1420] due to args.save_total_limit
 90%|█████████ | 1442/1600 [06:28<01:25,  1.85it/s] 90%|█████████ | 1445/1600 [06:28<01:00,  2.55it/s] 90%|█████████ | 1448/1600 [06:28<00:43,  3.48it/s] 91%|█████████ | 1451/1600 [06:28<00:31,  4.68it/s] 91%|█████████ | 1454/1600 [06:28<00:23,  6.15it/s] 91%|█████████ | 1457/1600 [06:29<00:18,  7.87it/s] 91%|█████████▏| 1460/1600 [06:29<00:13, 10.07it/s]                                                    91%|█████████▏| 1460/1600 [06:29<00:13, 10.07it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.02it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.96it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.35it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.39it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A                                                   
                                               [A 91%|█████████▏| 1460/1600 [06:30<00:13, 10.07it/s]
100%|██████████| 20/20 [00:00<00:00, 27.25it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1460
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1460/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1460/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1440] due to args.save_total_limit
 91%|█████████▏| 1463/1600 [06:33<01:12,  1.88it/s] 92%|█████████▏| 1466/1600 [06:33<00:51,  2.60it/s] 92%|█████████▏| 1469/1600 [06:34<00:36,  3.57it/s] 92%|█████████▏| 1472/1600 [06:34<00:26,  4.81it/s] 92%|█████████▏| 1475/1600 [06:34<00:19,  6.30it/s] 92%|█████████▏| 1478/1600 [06:34<00:15,  7.95it/s]                                                    92%|█████████▎| 1480/1600 [06:34<00:15,  7.95it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.34it/s][A
 40%|████      | 8/20 [00:00<00:00, 28.90it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.49it/s][A
 70%|███████   | 14/20 [00:00<00:00, 26.88it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 25.90it/s][A
100%|██████████| 20/20 [00:00<00:00, 26.74it/s][A                                                   
                                               [A 92%|█████████▎| 1480/1600 [06:35<00:15,  7.95it/s]
100%|██████████| 20/20 [00:00<00:00, 26.74it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1480
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1480/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1480/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1460] due to args.save_total_limit
 93%|█████████▎| 1481/1600 [06:39<01:06,  1.78it/s] 93%|█████████▎| 1484/1600 [06:39<00:47,  2.45it/s] 93%|█████████▎| 1487/1600 [06:39<00:33,  3.35it/s] 93%|█████████▎| 1490/1600 [06:39<00:24,  4.51it/s] 93%|█████████▎| 1493/1600 [06:39<00:17,  6.01it/s] 94%|█████████▎| 1496/1600 [06:39<00:13,  7.76it/s] 94%|█████████▎| 1499/1600 [06:39<00:10,  9.83it/s]                                                    94%|█████████▍| 1500/1600 [06:39<00:10,  9.83it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.99it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.43it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.08it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.41it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.32it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.12it/s][A                                                   
                                               [A 94%|█████████▍| 1500/1600 [06:40<00:10,  9.83it/s]
100%|██████████| 20/20 [00:00<00:00, 27.12it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1500
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1500/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1500/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1380] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1480] due to args.save_total_limit
 94%|█████████▍| 1502/1600 [06:44<00:53,  1.84it/s] 94%|█████████▍| 1505/1600 [06:44<00:37,  2.53it/s] 94%|█████████▍| 1508/1600 [06:44<00:26,  3.45it/s] 94%|█████████▍| 1511/1600 [06:45<00:19,  4.67it/s] 95%|█████████▍| 1514/1600 [06:45<00:13,  6.15it/s] 95%|█████████▍| 1517/1600 [06:45<00:10,  7.89it/s] 95%|█████████▌| 1520/1600 [06:45<00:07, 10.02it/s]                                                    95%|█████████▌| 1520/1600 [06:45<00:07, 10.02it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.98it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.45it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 29.09it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.41it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.27it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.18it/s][A                                                   
                                               [A 95%|█████████▌| 1520/1600 [06:46<00:07, 10.02it/s]
100%|██████████| 20/20 [00:00<00:00, 27.18it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1520
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1520/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1520/special_tokens_map.json
 95%|█████████▌| 1523/1600 [06:50<00:41,  1.87it/s] 95%|█████████▌| 1526/1600 [06:50<00:28,  2.57it/s] 96%|█████████▌| 1529/1600 [06:50<00:20,  3.51it/s] 96%|█████████▌| 1532/1600 [06:50<00:14,  4.73it/s] 96%|█████████▌| 1535/1600 [06:50<00:10,  6.19it/s] 96%|█████████▌| 1538/1600 [06:50<00:07,  7.94it/s]                                                    96%|█████████▋| 1540/1600 [06:50<00:07,  7.94it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.78it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.30it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.21it/s][A                                                   
                                               [A 96%|█████████▋| 1540/1600 [06:51<00:07,  7.94it/s]
100%|██████████| 20/20 [00:00<00:00, 26.21it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1540
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1540/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1540/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1500] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1520] due to args.save_total_limit
 96%|█████████▋| 1541/1600 [06:55<00:33,  1.76it/s] 96%|█████████▋| 1544/1600 [06:55<00:23,  2.43it/s] 97%|█████████▋| 1547/1600 [06:55<00:15,  3.32it/s] 97%|█████████▋| 1550/1600 [06:55<00:11,  4.47it/s] 97%|█████████▋| 1553/1600 [06:56<00:07,  5.92it/s] 97%|█████████▋| 1556/1600 [06:56<00:05,  7.69it/s] 97%|█████████▋| 1559/1600 [06:56<00:04,  9.77it/s]                                                    98%|█████████▊| 1560/1600 [06:56<00:04,  9.77it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.44it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.65it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.29it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.28it/s][A                                                   
                                               [A 98%|█████████▊| 1560/1600 [06:57<00:04,  9.77it/s]
100%|██████████| 20/20 [00:00<00:00, 26.28it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1560
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1560/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1560/special_tokens_map.json
 98%|█████████▊| 1562/1600 [07:00<00:19,  1.91it/s] 98%|█████████▊| 1565/1600 [07:00<00:13,  2.63it/s] 98%|█████████▊| 1568/1600 [07:01<00:08,  3.58it/s] 98%|█████████▊| 1571/1600 [07:01<00:06,  4.82it/s] 98%|█████████▊| 1574/1600 [07:01<00:04,  6.30it/s] 99%|█████████▊| 1577/1600 [07:01<00:02,  8.12it/s] 99%|█████████▉| 1580/1600 [07:01<00:01, 10.26it/s]                                                    99%|█████████▉| 1580/1600 [07:01<00:01, 10.26it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 34.83it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.35it/s][A
 55%|█████▌    | 11/20 [00:00<00:00, 28.91it/s][A
 70%|███████   | 14/20 [00:00<00:00, 27.32it/s][A
 85%|████████▌ | 17/20 [00:00<00:00, 26.33it/s][A
100%|██████████| 20/20 [00:00<00:00, 27.22it/s][A                                                   
                                               [A 99%|█████████▉| 1580/1600 [07:02<00:01, 10.26it/s]
100%|██████████| 20/20 [00:00<00:00, 27.22it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1580
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1580/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1580/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1540] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1560] due to args.save_total_limit
 99%|█████████▉| 1583/1600 [07:06<00:09,  1.84it/s] 99%|█████████▉| 1586/1600 [07:06<00:05,  2.54it/s] 99%|█████████▉| 1589/1600 [07:06<00:03,  3.48it/s]100%|█████████▉| 1592/1600 [07:06<00:01,  4.67it/s]100%|█████████▉| 1595/1600 [07:06<00:00,  6.16it/s]100%|█████████▉| 1598/1600 [07:06<00:00,  7.93it/s]                                                   100%|██████████| 1600/1600 [07:07<00:00,  7.93it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.01it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.45it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.79it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.41it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.36it/s][A                                                   
                                               [A100%|██████████| 1600/1600 [07:07<00:00,  7.93it/s]
100%|██████████| 20/20 [00:00<00:00, 26.36it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1600
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1600/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1600/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1580] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from /home/bhanuv/projects/classification_checkpoints/m2m100/checkpoint-1600 (score: 1.0453641414642334).
                                                   100%|██████████| 1600/1600 [07:12<00:00,  7.93it/s]100%|██████████| 1600/1600 [07:12<00:00,  3.70it/s]
