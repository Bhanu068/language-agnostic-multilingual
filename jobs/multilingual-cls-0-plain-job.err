Using custom data configuration default-language=en
Reusing dataset xnli (/home/bhanuv/.cache/huggingface/datasets/xnli/default-language=en/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd)
Some weights of the model checkpoint at facebook/m2m100_418M were not used when initializing M2M100Encoder: ['model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.8.fc1.weight', 'model.encoder.layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.6.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.encoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.fc1.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.decoder.layers.8.encoder_attn.k_proj.weight', 'model.decoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.11.encoder_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.3.fc2.bias', 'model.encoder.layers.5.fc1.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.8.encoder_attn.out_proj.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.2.fc2.weight', 'model.encoder.layers.8.fc1.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.9.fc1.weight', 'model.decoder.layers.11.encoder_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.encoder.layers.10.fc1.bias', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.q_proj.weight', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.decoder.layers.9.encoder_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.8.fc2.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.11.fc2.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.decoder.layers.5.fc1.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.7.fc2.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.2.fc2.weight', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.decoder.layers.10.encoder_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.decoder.layers.7.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.embed_tokens.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.10.encoder_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.6.fc1.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.6.fc1.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.6.encoder_attn.k_proj.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.11.fc1.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.fc1.weight', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.k_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.10.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.8.encoder_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.9.encoder_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.10.encoder_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.encoder.layers.11.fc1.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layer_norm.bias', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.fc1.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.10.encoder_attn.q_proj.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.11.encoder_attn_layer_norm.weight', 'model.decoder.layers.10.fc2.weight', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.8.fc2.bias', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.6.fc1.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.9.fc2.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.encoder_attn.k_proj.weight', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.8.fc2.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.encoder.layers.6.fc2.weight', 'model.decoder.layers.8.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.11.encoder_attn.out_proj.weight', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.10.fc2.weight', 'model.decoder.layers.11.encoder_attn_layer_norm.bias', 'model.decoder.layers.6.encoder_attn.v_proj.weight', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.encoder.layers.11.fc2.weight', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.4.fc1.weight', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.encoder.layers.5.fc2.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'lm_head.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.11.encoder_attn.q_proj.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.6.encoder_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.7.encoder_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.6.fc2.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layer_norm.weight', 'model.shared.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.encoder.layers.7.fc2.weight', 'model.decoder.layers.9.encoder_attn_layer_norm.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.9.encoder_attn.q_proj.weight', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.v_proj.weight']
- This IS expected if you are initializing M2M100Encoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing M2M100Encoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of M2M100Encoder were not initialized from the model checkpoint at facebook/m2m100_418M and are newly initialized: ['model.layers.11.self_attn_layer_norm.bias', 'model.layers.0.self_attn_layer_norm.weight', 'model.layers.0.final_layer_norm.weight', 'model.layers.7.self_attn_layer_norm.weight', 'model.layers.4.self_attn_layer_norm.bias', 'model.layers.6.self_attn.out_proj.weight', 'model.embed_tokens.weight', 'model.layers.1.final_layer_norm.bias', 'model.layers.0.fc1.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.final_layer_norm.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.9.fc2.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.3.final_layer_norm.weight', 'model.layers.7.self_attn.out_proj.weight', 'model.embed_positions.weights', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.7.fc1.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.8.self_attn.out_proj.bias', 'model.layers.2.self_attn_layer_norm.bias', 'model.layers.2.fc2.weight', 'model.layers.10.fc1.weight', 'model.layers.5.self_attn_layer_norm.bias', 'model.layers.2.fc1.bias', 'model.layers.1.self_attn.out_proj.bias', 'model.layers.4.fc2.weight', 'model.layers.10.self_attn.out_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.2.self_attn.out_proj.bias', 'model.layers.2.fc2.bias', 'model.layers.5.final_layer_norm.bias', 'model.layers.8.fc1.bias', 'model.layers.11.final_layer_norm.bias', 'model.layers.1.fc2.bias', 'model.layers.5.fc1.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.5.self_attn_layer_norm.weight', 'model.layers.9.self_attn_layer_norm.weight', 'model.layers.8.self_attn.out_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.0.self_attn_layer_norm.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.4.self_attn.out_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.6.fc1.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.fc1.weight', 'model.layers.5.fc1.weight', 'model.layers.9.fc2.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.4.self_attn.out_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.3.fc2.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn_layer_norm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.fc1.bias', 'model.layers.0.fc2.weight', 'model.layers.7.self_attn_layer_norm.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.out_proj.bias', 'model.layers.11.fc2.bias', 'model.layers.5.self_attn.out_proj.bias', 'model.layers.0.fc2.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.4.fc2.bias', 'model.layers.6.self_attn.out_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn_layer_norm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.2.self_attn.out_proj.weight', 'model.layers.6.self_attn_layer_norm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.5.fc2.weight', 'model.layers.11.fc2.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.6.fc1.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.1.fc2.weight', 'model.layers.6.final_layer_norm.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.3.self_attn_layer_norm.weight', 'model.layers.8.fc2.bias', 'model.layers.10.fc2.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.9.self_attn.out_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn_layer_norm.bias', 'model.layers.10.final_layer_norm.weight', 'model.layers.8.self_attn_layer_norm.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.3.final_layer_norm.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.1.self_attn.out_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn_layer_norm.weight', 'model.layers.7.fc2.weight', 'model.layers.0.final_layer_norm.bias', 'model.layers.8.final_layer_norm.bias', 'model.layers.10.fc2.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layer_norm.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.11.fc1.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.9.final_layer_norm.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.11.self_attn_layer_norm.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.11.fc1.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.9.fc1.bias', 'model.layers.10.final_layer_norm.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.11.final_layer_norm.weight', 'model.layers.10.fc1.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.3.fc1.weight', 'model.layers.0.self_attn.out_proj.weight', 'model.layers.2.final_layer_norm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.fc1.weight', 'model.layers.6.self_attn_layer_norm.bias', 'model.layers.3.self_attn.out_proj.weight', 'model.layers.1.self_attn_layer_norm.weight', 'model.layers.7.fc2.bias', 'model.layers.7.final_layer_norm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.3.fc1.bias', 'model.layers.4.fc1.bias', 'model.layers.4.fc1.weight', 'model.layers.2.final_layer_norm.bias', 'model.layers.8.final_layer_norm.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.9.self_attn.out_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.8.fc2.weight', 'model.layers.0.fc1.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.6.final_layer_norm.weight', 'model.layers.5.final_layer_norm.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.10.self_attn.out_proj.bias', 'model.layers.10.self_attn_layer_norm.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.1.final_layer_norm.weight', 'model.layers.6.fc2.bias', 'model.layers.5.fc2.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.1.self_attn_layer_norm.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.out_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.9.final_layer_norm.bias', 'model.layers.11.self_attn.out_proj.weight', 'model.layers.3.self_attn_layer_norm.bias', 'model.layers.5.self_attn.out_proj.weight', 'model.layers.4.final_layer_norm.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.6.fc2.weight', 'model.layers.2.fc1.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layer_norm.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.0.self_attn.out_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.4.final_layer_norm.weight', 'model.layers.8.fc1.weight', 'model.layers.9.fc1.weight', 'model.layers.11.self_attn.out_proj.bias', 'model.layers.3.fc2.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.8.self_attn_layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /home/bhanuv/.cache/huggingface/datasets/xnli/default-language=en/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd/cache-215151111e50e1cd.arrow
Using amp half precision backend
The following columns in the training set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 2490
  Num Epochs = 80
  Instantaneous batch size per device = 128
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 1600
  0%|          | 0/1600 [00:00<?, ?it/s]  0%|          | 1/1600 [00:00<06:21,  4.19it/s]  0%|          | 4/1600 [00:00<02:07, 12.56it/s]  0%|          | 7/1600 [00:00<01:33, 17.08it/s]  1%|          | 10/1600 [00:00<01:20, 19.79it/s]  1%|          | 13/1600 [00:00<01:12, 21.81it/s]  1%|          | 16/1600 [00:00<01:09, 22.67it/s]  1%|          | 19/1600 [00:00<01:06, 23.74it/s]                                                   1%|▏         | 20/1600 [00:01<01:06, 23.74it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.64it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.03it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.97it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.88it/s][A                                                 
                                               [A  1%|▏         | 20/1600 [00:01<01:06, 23.74it/s]
100%|██████████| 20/20 [00:00<00:00, 26.88it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-20
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-20/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-20/special_tokens_map.json
  1%|▏         | 22/1600 [00:05<13:39,  1.93it/s]  2%|▏         | 25/1600 [00:05<09:40,  2.71it/s]  2%|▏         | 28/1600 [00:05<06:59,  3.75it/s]  2%|▏         | 31/1600 [00:05<05:10,  5.06it/s]  2%|▏         | 34/1600 [00:06<03:55,  6.66it/s]  2%|▏         | 37/1600 [00:06<03:03,  8.51it/s]                                                   2%|▎         | 40/1600 [00:06<03:03,  8.51it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.83it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.42it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.99it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.91it/s][A                                                 
                                               [A  2%|▎         | 40/1600 [00:07<03:03,  8.51it/s]
100%|██████████| 20/20 [00:00<00:00, 26.91it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-40
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-40/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-40/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-20] due to args.save_total_limit
  3%|▎         | 41/1600 [00:10<13:00,  2.00it/s]  3%|▎         | 44/1600 [00:10<09:39,  2.69it/s]  3%|▎         | 47/1600 [00:11<07:12,  3.59it/s]  3%|▎         | 50/1600 [00:11<05:25,  4.76it/s]  3%|▎         | 53/1600 [00:11<04:06,  6.27it/s]  4%|▎         | 56/1600 [00:11<03:12,  8.03it/s]  4%|▎         | 59/1600 [00:11<02:34, 10.00it/s]                                                   4%|▍         | 60/1600 [00:11<02:33, 10.00it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.83it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.10it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.45it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 28.01it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.91it/s][A/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                 
                                               [A  4%|▍         | 60/1600 [00:12<02:33, 10.00it/s]
100%|██████████| 20/20 [00:00<00:00, 26.91it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-60
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-60/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-60/special_tokens_map.json
  4%|▍         | 62/1600 [00:16<13:06,  1.95it/s]  4%|▍         | 65/1600 [00:16<09:30,  2.69it/s]  4%|▍         | 68/1600 [00:16<06:59,  3.65it/s]  4%|▍         | 71/1600 [00:16<05:12,  4.89it/s]  5%|▍         | 74/1600 [00:16<03:57,  6.43it/s]  5%|▍         | 77/1600 [00:16<03:05,  8.20it/s]  5%|▌         | 80/1600 [00:16<02:25, 10.44it/s]                                                   5%|▌         | 80/1600 [00:16<02:25, 10.44it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.76it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.10it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.39it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.84it/s][A                                                 
                                               [A  5%|▌         | 80/1600 [00:17<02:25, 10.44it/s]
100%|██████████| 20/20 [00:00<00:00, 26.84it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-80
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-80/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-80/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-40] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-60] due to args.save_total_limit
  5%|▌         | 83/1600 [00:21<14:00,  1.80it/s]  5%|▌         | 86/1600 [00:21<10:07,  2.49it/s]  6%|▌         | 89/1600 [00:21<07:21,  3.42it/s]  6%|▌         | 92/1600 [00:21<05:27,  4.61it/s]  6%|▌         | 95/1600 [00:22<04:08,  6.06it/s]  6%|▌         | 98/1600 [00:22<03:10,  7.88it/s]                                                   6%|▋         | 100/1600 [00:22<03:10,  7.88it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.88it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.12it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.46it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 28.03it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.93it/s][A                                                  
                                               [A  6%|▋         | 100/1600 [00:23<03:10,  7.88it/s]
100%|██████████| 20/20 [00:00<00:00, 26.93it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-100
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-100/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-100/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-80] due to args.save_total_limit
  6%|▋         | 101/1600 [00:26<13:31,  1.85it/s]  6%|▋         | 103/1600 [00:26<10:47,  2.31it/s]  7%|▋         | 106/1600 [00:26<07:36,  3.27it/s]  7%|▋         | 109/1600 [00:27<05:31,  4.50it/s]  7%|▋         | 112/1600 [00:27<04:05,  6.07it/s]  7%|▋         | 115/1600 [00:27<03:09,  7.84it/s]  7%|▋         | 118/1600 [00:27<02:27, 10.06it/s]                                                    8%|▊         | 120/1600 [00:27<02:27, 10.06it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.87it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.42it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.92it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.84it/s][A                                                  
                                               [A  8%|▊         | 120/1600 [00:28<02:27, 10.06it/s]
100%|██████████| 20/20 [00:00<00:00, 26.84it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-120
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-120/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-120/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-100] due to args.save_total_limit
  8%|▊         | 121/1600 [00:32<13:16,  1.86it/s]  8%|▊         | 124/1600 [00:32<09:36,  2.56it/s]  8%|▊         | 127/1600 [00:32<07:00,  3.51it/s]  8%|▊         | 130/1600 [00:32<05:10,  4.74it/s]  8%|▊         | 133/1600 [00:32<03:53,  6.27it/s]  8%|▊         | 136/1600 [00:32<03:00,  8.12it/s]  9%|▊         | 139/1600 [00:32<02:22, 10.23it/s]                                                    9%|▉         | 140/1600 [00:32<02:22, 10.23it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.83it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.89it/s][A                                                  
                                               [A  9%|▉         | 140/1600 [00:33<02:22, 10.23it/s]
100%|██████████| 20/20 [00:00<00:00, 26.89it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-140
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-140/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-140/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-120] due to args.save_total_limit
  9%|▉         | 142/1600 [00:37<12:58,  1.87it/s]  9%|▉         | 145/1600 [00:37<09:21,  2.59it/s]  9%|▉         | 148/1600 [00:37<06:48,  3.55it/s]  9%|▉         | 151/1600 [00:37<05:02,  4.78it/s] 10%|▉         | 154/1600 [00:37<03:47,  6.36it/s] 10%|▉         | 157/1600 [00:38<02:55,  8.23it/s] 10%|█         | 160/1600 [00:38<02:16, 10.51it/s]                                                   10%|█         | 160/1600 [00:38<02:16, 10.51it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.82it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.10it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.45it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.86it/s][A/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/bhanuv/python_envs/multiling/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                  
                                               [A 10%|█         | 160/1600 [00:39<02:16, 10.51it/s]
100%|██████████| 20/20 [00:00<00:00, 26.86it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-160
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-160/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-160/special_tokens_map.json
 10%|█         | 163/1600 [00:42<12:15,  1.96it/s] 10%|█         | 166/1600 [00:42<08:53,  2.69it/s] 11%|█         | 169/1600 [00:42<06:32,  3.65it/s] 11%|█         | 172/1600 [00:42<04:49,  4.93it/s] 11%|█         | 175/1600 [00:43<03:38,  6.51it/s] 11%|█         | 178/1600 [00:43<02:50,  8.36it/s]                                                   11%|█▏        | 180/1600 [00:43<02:49,  8.36it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.69it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.03it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.90it/s][A                                                  
                                               [A 11%|█▏        | 180/1600 [00:44<02:49,  8.36it/s]
100%|██████████| 20/20 [00:00<00:00, 26.90it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-180
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-180/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-180/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-160] due to args.save_total_limit
 11%|█▏        | 181/1600 [00:47<12:41,  1.86it/s] 11%|█▏        | 183/1600 [00:47<10:08,  2.33it/s] 12%|█▏        | 186/1600 [00:48<07:11,  3.28it/s] 12%|█▏        | 189/1600 [00:48<05:11,  4.53it/s] 12%|█▏        | 192/1600 [00:48<03:52,  6.05it/s] 12%|█▏        | 195/1600 [00:48<02:58,  7.88it/s] 12%|█▏        | 198/1600 [00:48<02:20,  9.97it/s]                                                   12%|█▎        | 200/1600 [00:48<02:20,  9.97it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.85it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.10it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.44it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.89it/s][A                                                  
                                               [A 12%|█▎        | 200/1600 [00:49<02:20,  9.97it/s]
100%|██████████| 20/20 [00:00<00:00, 26.89it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-200
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-200/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-140] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-180] due to args.save_total_limit
 13%|█▎        | 201/1600 [00:53<13:01,  1.79it/s] 13%|█▎        | 204/1600 [00:53<09:22,  2.48it/s] 13%|█▎        | 207/1600 [00:53<06:49,  3.40it/s] 13%|█▎        | 210/1600 [00:53<05:04,  4.57it/s] 13%|█▎        | 213/1600 [00:53<03:48,  6.08it/s] 14%|█▎        | 216/1600 [00:53<02:57,  7.79it/s] 14%|█▎        | 219/1600 [00:54<02:20,  9.86it/s]                                                   14%|█▍        | 220/1600 [00:54<02:19,  9.86it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.84it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.11it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.45it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 28.01it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.92it/s][A                                                  
                                               [A 14%|█▍        | 220/1600 [00:55<02:19,  9.86it/s]
100%|██████████| 20/20 [00:00<00:00, 26.92it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-220
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-220/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-220/special_tokens_map.json
 14%|█▍        | 222/1600 [00:58<11:38,  1.97it/s] 14%|█▍        | 225/1600 [00:58<08:27,  2.71it/s] 14%|█▍        | 228/1600 [00:58<06:10,  3.70it/s] 14%|█▍        | 231/1600 [00:58<04:36,  4.95it/s] 15%|█▍        | 234/1600 [00:58<03:28,  6.56it/s] 15%|█▍        | 237/1600 [00:59<02:40,  8.48it/s]                                                   15%|█▌        | 240/1600 [00:59<02:40,  8.48it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.89it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.13it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.47it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 28.03it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.89it/s][A                                                  
                                               [A 15%|█▌        | 240/1600 [01:00<02:40,  8.48it/s]
100%|██████████| 20/20 [00:00<00:00, 26.89it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-240
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-240/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-240/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-220] due to args.save_total_limit
 15%|█▌        | 241/1600 [01:03<11:20,  2.00it/s] 15%|█▌        | 244/1600 [01:03<08:24,  2.69it/s] 15%|█▌        | 247/1600 [01:03<06:14,  3.61it/s] 16%|█▌        | 250/1600 [01:04<04:40,  4.81it/s] 16%|█▌        | 253/1600 [01:04<03:35,  6.26it/s] 16%|█▌        | 256/1600 [01:04<02:48,  8.00it/s] 16%|█▌        | 259/1600 [01:04<02:13, 10.02it/s]                                                   16%|█▋        | 260/1600 [01:04<02:13, 10.02it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.83it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.11it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.36it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.87it/s][A                                                  
                                               [A 16%|█▋        | 260/1600 [01:05<02:13, 10.02it/s]
100%|██████████| 20/20 [00:00<00:00, 26.87it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-260
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-260/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-260/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-240] due to args.save_total_limit
 16%|█▋        | 262/1600 [01:08<11:34,  1.93it/s] 17%|█▋        | 265/1600 [01:09<08:24,  2.64it/s] 17%|█▋        | 268/1600 [01:09<06:09,  3.61it/s] 17%|█▋        | 271/1600 [01:09<04:33,  4.86it/s] 17%|█▋        | 274/1600 [01:09<03:28,  6.35it/s] 17%|█▋        | 277/1600 [01:09<02:42,  8.15it/s]                                                   18%|█▊        | 280/1600 [01:09<02:42,  8.15it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.82it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.09it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.43it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 28.00it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.90it/s][A                                                  
                                               [A 18%|█▊        | 280/1600 [01:10<02:42,  8.15it/s]
100%|██████████| 20/20 [00:00<00:00, 26.90it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-280
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-280/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-280/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-200] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-260] due to args.save_total_limit
 18%|█▊        | 281/1600 [01:14<11:28,  1.92it/s] 18%|█▊        | 284/1600 [01:14<08:31,  2.57it/s] 18%|█▊        | 287/1600 [01:14<06:19,  3.46it/s] 18%|█▊        | 290/1600 [01:14<04:44,  4.61it/s] 18%|█▊        | 293/1600 [01:15<03:36,  6.04it/s] 18%|█▊        | 296/1600 [01:15<02:47,  7.78it/s] 19%|█▊        | 299/1600 [01:15<02:11,  9.87it/s]                                                   19%|█▉        | 300/1600 [01:15<02:11,  9.87it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.85it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.11it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.45it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 28.01it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.83it/s][A                                                  
                                               [A 19%|█▉        | 300/1600 [01:16<02:11,  9.87it/s]
100%|██████████| 20/20 [00:00<00:00, 26.83it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-300
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-300/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-300/special_tokens_map.json
 19%|█▉        | 302/1600 [01:19<10:56,  1.98it/s] 19%|█▉        | 305/1600 [01:19<07:54,  2.73it/s] 19%|█▉        | 308/1600 [01:19<05:47,  3.72it/s] 19%|█▉        | 311/1600 [01:19<04:19,  4.97it/s] 20%|█▉        | 314/1600 [01:20<03:16,  6.54it/s] 20%|█▉        | 317/1600 [01:20<02:32,  8.39it/s] 20%|██        | 320/1600 [01:20<01:59, 10.70it/s]                                                   20%|██        | 320/1600 [01:20<01:59, 10.70it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.84it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.10it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.44it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 28.00it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.88it/s][A                                                  
                                               [A 20%|██        | 320/1600 [01:21<01:59, 10.70it/s]
100%|██████████| 20/20 [00:00<00:00, 26.88it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-320
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-320/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-320/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-280] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-300] due to args.save_total_limit
 20%|██        | 323/1600 [01:25<11:42,  1.82it/s] 20%|██        | 326/1600 [01:25<08:26,  2.52it/s] 21%|██        | 329/1600 [01:25<06:10,  3.43it/s] 21%|██        | 332/1600 [01:25<04:34,  4.61it/s] 21%|██        | 335/1600 [01:25<03:27,  6.10it/s] 21%|██        | 338/1600 [01:25<02:41,  7.83it/s]                                                   21%|██▏       | 340/1600 [01:25<02:40,  7.83it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.83it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.39it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.72it/s][A                                                  
                                               [A 21%|██▏       | 340/1600 [01:26<02:40,  7.83it/s]
100%|██████████| 20/20 [00:00<00:00, 26.72it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-340
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-340/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-340/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-320] due to args.save_total_limit
 21%|██▏       | 341/1600 [01:30<11:27,  1.83it/s] 22%|██▏       | 344/1600 [01:30<08:17,  2.52it/s] 22%|██▏       | 347/1600 [01:30<06:02,  3.45it/s] 22%|██▏       | 350/1600 [01:30<04:27,  4.68it/s] 22%|██▏       | 353/1600 [01:30<03:21,  6.19it/s] 22%|██▏       | 356/1600 [01:30<02:34,  8.07it/s] 22%|██▏       | 359/1600 [01:31<02:03, 10.09it/s]                                                   22%|██▎       | 360/1600 [01:31<02:02, 10.09it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.73it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.39it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.86it/s][A                                                  
                                               [A 22%|██▎       | 360/1600 [01:32<02:02, 10.09it/s]
100%|██████████| 20/20 [00:00<00:00, 26.86it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-360
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-360/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-360/special_tokens_map.json
 23%|██▎       | 362/1600 [01:35<10:44,  1.92it/s] 23%|██▎       | 365/1600 [01:35<07:45,  2.65it/s] 23%|██▎       | 368/1600 [01:35<05:40,  3.62it/s] 23%|██▎       | 371/1600 [01:35<04:13,  4.86it/s] 23%|██▎       | 374/1600 [01:36<03:11,  6.41it/s] 24%|██▎       | 377/1600 [01:36<02:30,  8.15it/s] 24%|██▍       | 380/1600 [01:36<01:57, 10.40it/s]                                                   24%|██▍       | 380/1600 [01:36<01:57, 10.40it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.71it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.01it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.37it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.91it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.81it/s][A                                                  
                                               [A 24%|██▍       | 380/1600 [01:37<01:57, 10.40it/s]
100%|██████████| 20/20 [00:00<00:00, 26.81it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-380
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-380/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-380/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-360] due to args.save_total_limit
 24%|██▍       | 383/1600 [01:40<10:36,  1.91it/s] 24%|██▍       | 386/1600 [01:41<07:39,  2.64it/s] 24%|██▍       | 389/1600 [01:41<05:37,  3.59it/s] 24%|██▍       | 392/1600 [01:41<04:10,  4.82it/s] 25%|██▍       | 395/1600 [01:41<03:10,  6.34it/s] 25%|██▍       | 398/1600 [01:41<02:26,  8.18it/s]                                                   25%|██▌       | 400/1600 [01:41<02:26,  8.18it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.76it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.84it/s][A                                                  
                                               [A 25%|██▌       | 400/1600 [01:42<02:26,  8.18it/s]
100%|██████████| 20/20 [00:00<00:00, 26.84it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-400
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-400/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-340] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-380] due to args.save_total_limit
 25%|██▌       | 401/1600 [01:46<11:17,  1.77it/s] 25%|██▌       | 404/1600 [01:46<08:08,  2.45it/s] 25%|██▌       | 407/1600 [01:46<05:56,  3.34it/s] 26%|██▌       | 410/1600 [01:46<04:25,  4.49it/s] 26%|██▌       | 413/1600 [01:46<03:20,  5.93it/s] 26%|██▌       | 416/1600 [01:46<02:32,  7.75it/s] 26%|██▌       | 419/1600 [01:47<02:01,  9.75it/s]                                                   26%|██▋       | 420/1600 [01:47<02:01,  9.75it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.75it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.05it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.97it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.89it/s][A                                                  
                                               [A 26%|██▋       | 420/1600 [01:48<02:01,  9.75it/s]
100%|██████████| 20/20 [00:00<00:00, 26.89it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-420
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-420/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-420/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-400] due to args.save_total_limit
 26%|██▋       | 422/1600 [01:51<10:30,  1.87it/s] 27%|██▋       | 425/1600 [01:51<07:34,  2.58it/s] 27%|██▋       | 428/1600 [01:51<05:32,  3.53it/s] 27%|██▋       | 431/1600 [01:52<04:06,  4.74it/s] 27%|██▋       | 434/1600 [01:52<03:05,  6.29it/s] 27%|██▋       | 437/1600 [01:52<02:24,  8.06it/s] 28%|██▊       | 440/1600 [01:52<01:52, 10.29it/s]                                                   28%|██▊       | 440/1600 [01:52<01:52, 10.29it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.78it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.03it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.38it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.79it/s][A                                                  
                                               [A 28%|██▊       | 440/1600 [01:53<01:52, 10.29it/s]
100%|██████████| 20/20 [00:00<00:00, 26.79it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-440
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-440/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-440/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-420] due to args.save_total_limit
 28%|██▊       | 443/1600 [01:57<11:05,  1.74it/s] 28%|██▊       | 446/1600 [01:57<07:59,  2.40it/s] 28%|██▊       | 449/1600 [01:57<05:48,  3.30it/s] 28%|██▊       | 452/1600 [01:57<04:17,  4.45it/s] 28%|██▊       | 455/1600 [01:57<03:14,  5.89it/s] 29%|██▊       | 458/1600 [01:58<02:28,  7.68it/s]                                                   29%|██▉       | 460/1600 [01:58<02:28,  7.68it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.81it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.97it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.82it/s][A                                                  
                                               [A 29%|██▉       | 460/1600 [01:59<02:28,  7.68it/s]
100%|██████████| 20/20 [00:00<00:00, 26.82it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-460
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-460/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-460/special_tokens_map.json
 29%|██▉       | 461/1600 [02:02<10:12,  1.86it/s] 29%|██▉       | 464/1600 [02:02<07:23,  2.56it/s] 29%|██▉       | 467/1600 [02:02<05:24,  3.50it/s] 29%|██▉       | 470/1600 [02:02<04:00,  4.70it/s] 30%|██▉       | 473/1600 [02:03<03:01,  6.20it/s] 30%|██▉       | 476/1600 [02:03<02:19,  8.06it/s] 30%|██▉       | 479/1600 [02:03<01:52,  9.99it/s]                                                   30%|███       | 480/1600 [02:03<01:52,  9.99it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.84it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.97it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.88it/s][A                                                  
                                               [A 30%|███       | 480/1600 [02:04<01:52,  9.99it/s]
100%|██████████| 20/20 [00:00<00:00, 26.88it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-480
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-480/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-480/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-460] due to args.save_total_limit
 30%|███       | 482/1600 [02:07<09:56,  1.87it/s] 30%|███       | 485/1600 [02:08<07:10,  2.59it/s] 30%|███       | 488/1600 [02:08<05:16,  3.52it/s] 31%|███       | 491/1600 [02:08<03:53,  4.76it/s] 31%|███       | 494/1600 [02:08<02:55,  6.32it/s] 31%|███       | 497/1600 [02:08<02:16,  8.06it/s] 31%|███▏      | 500/1600 [02:08<01:47, 10.26it/s]                                                   31%|███▏      | 500/1600 [02:08<01:47, 10.26it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.59it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.93it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.32it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.88it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.72it/s][A                                                  
                                               [A 31%|███▏      | 500/1600 [02:09<01:47, 10.26it/s]
100%|██████████| 20/20 [00:00<00:00, 26.72it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-500
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-500/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-440] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-480] due to args.save_total_limit
 31%|███▏      | 503/1600 [02:13<10:11,  1.79it/s] 32%|███▏      | 506/1600 [02:13<07:20,  2.48it/s] 32%|███▏      | 509/1600 [02:13<05:20,  3.40it/s] 32%|███▏      | 512/1600 [02:13<03:56,  4.60it/s] 32%|███▏      | 515/1600 [02:14<02:57,  6.10it/s] 32%|███▏      | 518/1600 [02:14<02:16,  7.92it/s]                                                   32%|███▎      | 520/1600 [02:14<02:16,  7.92it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.81it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.87it/s][A                                                  
                                               [A 32%|███▎      | 520/1600 [02:15<02:16,  7.92it/s]
100%|██████████| 20/20 [00:00<00:00, 26.87it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-520
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-520/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-520/special_tokens_map.json
 33%|███▎      | 521/1600 [02:18<09:49,  1.83it/s] 33%|███▎      | 524/1600 [02:18<07:05,  2.53it/s] 33%|███▎      | 527/1600 [02:19<05:10,  3.45it/s] 33%|███▎      | 530/1600 [02:19<03:49,  4.66it/s] 33%|███▎      | 533/1600 [02:19<02:52,  6.17it/s] 34%|███▎      | 536/1600 [02:19<02:14,  7.93it/s] 34%|███▎      | 539/1600 [02:19<01:45, 10.10it/s]                                                   34%|███▍      | 540/1600 [02:19<01:44, 10.10it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.78it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.97it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.88it/s][A                                                  
                                               [A 34%|███▍      | 540/1600 [02:20<01:44, 10.10it/s]
100%|██████████| 20/20 [00:00<00:00, 26.88it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-540
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-540/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-540/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-520] due to args.save_total_limit
 34%|███▍      | 542/1600 [02:24<09:16,  1.90it/s] 34%|███▍      | 545/1600 [02:24<06:40,  2.63it/s] 34%|███▍      | 548/1600 [02:24<04:51,  3.60it/s] 34%|███▍      | 551/1600 [02:24<03:35,  4.86it/s] 35%|███▍      | 554/1600 [02:24<02:44,  6.37it/s] 35%|███▍      | 557/1600 [02:24<02:07,  8.21it/s] 35%|███▌      | 560/1600 [02:24<01:39, 10.47it/s]                                                   35%|███▌      | 560/1600 [02:24<01:39, 10.47it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.98it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.25it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.80it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.77it/s][A                                                  
                                               [A 35%|███▌      | 560/1600 [02:25<01:39, 10.47it/s]
100%|██████████| 20/20 [00:00<00:00, 26.77it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-560
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-560/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-560/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-500] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-540] due to args.save_total_limit
 35%|███▌      | 563/1600 [02:29<09:31,  1.81it/s] 35%|███▌      | 566/1600 [02:29<06:53,  2.50it/s] 36%|███▌      | 569/1600 [02:29<05:00,  3.43it/s] 36%|███▌      | 572/1600 [02:29<03:41,  4.64it/s] 36%|███▌      | 575/1600 [02:30<02:47,  6.13it/s] 36%|███▌      | 578/1600 [02:30<02:09,  7.91it/s]                                                   36%|███▋      | 580/1600 [02:30<02:09,  7.91it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.79it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.97it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.87it/s][A                                                  
                                               [A 36%|███▋      | 580/1600 [02:31<02:09,  7.91it/s]
100%|██████████| 20/20 [00:00<00:00, 26.87it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-580
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-580/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-580/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-560] due to args.save_total_limit
 36%|███▋      | 581/1600 [02:34<09:18,  1.83it/s] 36%|███▋      | 584/1600 [02:34<06:43,  2.52it/s] 37%|███▋      | 587/1600 [02:35<04:53,  3.45it/s] 37%|███▋      | 590/1600 [02:35<03:37,  4.64it/s] 37%|███▋      | 593/1600 [02:35<02:45,  6.10it/s] 37%|███▋      | 596/1600 [02:35<02:07,  7.88it/s] 37%|███▋      | 599/1600 [02:35<01:40,  9.99it/s]                                                   38%|███▊      | 600/1600 [02:35<01:40,  9.99it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.73it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.01it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.35it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.92it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.76it/s][A                                                  
                                               [A 38%|███▊      | 600/1600 [02:36<01:40,  9.99it/s]
100%|██████████| 20/20 [00:00<00:00, 26.76it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-600
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-600/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-600/special_tokens_map.json
 38%|███▊      | 602/1600 [02:40<08:41,  1.91it/s] 38%|███▊      | 605/1600 [02:40<06:17,  2.63it/s] 38%|███▊      | 608/1600 [02:40<04:35,  3.60it/s] 38%|███▊      | 611/1600 [02:40<03:25,  4.82it/s] 38%|███▊      | 614/1600 [02:40<02:35,  6.36it/s] 39%|███▊      | 617/1600 [02:40<02:00,  8.19it/s] 39%|███▉      | 620/1600 [02:40<01:34, 10.38it/s]                                                   39%|███▉      | 620/1600 [02:40<01:34, 10.38it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.77it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.05it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.87it/s][A                                                  
                                               [A 39%|███▉      | 620/1600 [02:41<01:34, 10.38it/s]
100%|██████████| 20/20 [00:00<00:00, 26.87it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-620
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-620/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-620/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-600] due to args.save_total_limit
 39%|███▉      | 623/1600 [02:45<08:28,  1.92it/s] 39%|███▉      | 626/1600 [02:45<06:06,  2.66it/s] 39%|███▉      | 629/1600 [02:45<04:27,  3.64it/s] 40%|███▉      | 632/1600 [02:45<03:19,  4.86it/s] 40%|███▉      | 635/1600 [02:45<02:29,  6.45it/s] 40%|███▉      | 638/1600 [02:45<01:55,  8.32it/s]                                                   40%|████      | 640/1600 [02:46<01:55,  8.32it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.74it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.39it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.87it/s][A                                                  
                                               [A 40%|████      | 640/1600 [02:46<01:55,  8.32it/s]
100%|██████████| 20/20 [00:00<00:00, 26.87it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-640
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-640/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-640/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-580] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-620] due to args.save_total_limit
 40%|████      | 641/1600 [02:50<09:02,  1.77it/s] 40%|████      | 644/1600 [02:50<06:32,  2.43it/s] 40%|████      | 647/1600 [02:51<04:45,  3.34it/s] 41%|████      | 650/1600 [02:51<03:30,  4.51it/s] 41%|████      | 653/1600 [02:51<02:39,  5.95it/s] 41%|████      | 656/1600 [02:51<02:03,  7.66it/s] 41%|████      | 659/1600 [02:51<01:38,  9.58it/s]                                                   41%|████▏     | 660/1600 [02:51<01:38,  9.58it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.42it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.88it/s][A                                                  
                                               [A 41%|████▏     | 660/1600 [02:52<01:38,  9.58it/s]
100%|██████████| 20/20 [00:00<00:00, 26.88it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-660
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-660/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-660/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-640] due to args.save_total_limit
 41%|████▏     | 662/1600 [02:56<08:23,  1.86it/s] 42%|████▏     | 665/1600 [02:56<06:04,  2.57it/s] 42%|████▏     | 668/1600 [02:56<04:26,  3.50it/s] 42%|████▏     | 671/1600 [02:56<03:15,  4.74it/s] 42%|████▏     | 674/1600 [02:56<02:28,  6.22it/s] 42%|████▏     | 677/1600 [02:56<01:55,  7.97it/s] 42%|████▎     | 680/1600 [02:56<01:30, 10.19it/s]                                                   42%|████▎     | 680/1600 [02:56<01:30, 10.19it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.50it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.96it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.35it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.81it/s][A                                                  
                                               [A 42%|████▎     | 680/1600 [02:57<01:30, 10.19it/s]
100%|██████████| 20/20 [00:00<00:00, 26.81it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-680
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-680/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-680/special_tokens_map.json
 43%|████▎     | 683/1600 [03:01<07:56,  1.92it/s] 43%|████▎     | 686/1600 [03:01<05:43,  2.66it/s] 43%|████▎     | 689/1600 [03:01<04:10,  3.64it/s] 43%|████▎     | 692/1600 [03:01<03:06,  4.87it/s] 43%|████▎     | 695/1600 [03:01<02:21,  6.39it/s] 44%|████▎     | 698/1600 [03:02<01:49,  8.26it/s]                                                   44%|████▍     | 700/1600 [03:02<01:48,  8.26it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.70it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.85it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.27it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.89it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.80it/s][A                                                  
                                               [A 44%|████▍     | 700/1600 [03:03<01:48,  8.26it/s]
100%|██████████| 20/20 [00:00<00:00, 26.80it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-700
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-700/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-660] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-680] due to args.save_total_limit
 44%|████▍     | 701/1600 [03:06<08:28,  1.77it/s] 44%|████▍     | 704/1600 [03:06<06:07,  2.44it/s] 44%|████▍     | 707/1600 [03:07<04:28,  3.33it/s] 44%|████▍     | 710/1600 [03:07<03:18,  4.47it/s] 45%|████▍     | 713/1600 [03:07<02:28,  5.98it/s] 45%|████▍     | 716/1600 [03:07<01:54,  7.71it/s] 45%|████▍     | 719/1600 [03:07<01:30,  9.76it/s]                                                   45%|████▌     | 720/1600 [03:07<01:30,  9.76it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.74it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.38it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.92it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.76it/s][A                                                  
                                               [A 45%|████▌     | 720/1600 [03:08<01:30,  9.76it/s]
100%|██████████| 20/20 [00:00<00:00, 26.76it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-720
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-720/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-720/special_tokens_map.json
 45%|████▌     | 722/1600 [03:12<07:34,  1.93it/s] 45%|████▌     | 725/1600 [03:12<05:29,  2.65it/s] 46%|████▌     | 728/1600 [03:12<04:01,  3.62it/s] 46%|████▌     | 731/1600 [03:12<02:58,  4.86it/s] 46%|████▌     | 734/1600 [03:12<02:15,  6.41it/s] 46%|████▌     | 737/1600 [03:12<01:45,  8.21it/s] 46%|████▋     | 740/1600 [03:12<01:22, 10.43it/s]                                                   46%|████▋     | 740/1600 [03:12<01:22, 10.43it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.74it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.05it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.97it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.82it/s][A                                                  
                                               [A 46%|████▋     | 740/1600 [03:13<01:22, 10.43it/s]
100%|██████████| 20/20 [00:00<00:00, 26.82it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-740
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-740/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-740/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-720] due to args.save_total_limit
 46%|████▋     | 743/1600 [03:17<07:27,  1.92it/s] 47%|████▋     | 746/1600 [03:17<05:24,  2.63it/s] 47%|████▋     | 749/1600 [03:17<03:56,  3.59it/s] 47%|████▋     | 752/1600 [03:17<02:55,  4.83it/s] 47%|████▋     | 755/1600 [03:17<02:13,  6.35it/s] 47%|████▋     | 758/1600 [03:18<01:42,  8.20it/s]                                                   48%|████▊     | 760/1600 [03:18<01:42,  8.20it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.81it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.43it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.85it/s][A                                                  
                                               [A 48%|████▊     | 760/1600 [03:19<01:42,  8.20it/s]
100%|██████████| 20/20 [00:00<00:00, 26.85it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-760
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-760/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-760/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-740] due to args.save_total_limit
 48%|████▊     | 761/1600 [03:22<08:03,  1.73it/s] 48%|████▊     | 764/1600 [03:23<05:48,  2.40it/s] 48%|████▊     | 767/1600 [03:23<04:14,  3.28it/s] 48%|████▊     | 770/1600 [03:23<03:06,  4.45it/s] 48%|████▊     | 773/1600 [03:23<02:20,  5.87it/s] 48%|████▊     | 776/1600 [03:23<01:48,  7.58it/s] 49%|████▊     | 779/1600 [03:23<01:26,  9.45it/s]                                                   49%|████▉     | 780/1600 [03:23<01:26,  9.45it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.78it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.95it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.82it/s][A                                                  
                                               [A 49%|████▉     | 780/1600 [03:24<01:26,  9.45it/s]
100%|██████████| 20/20 [00:00<00:00, 26.82it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-780
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-780/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-780/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-760] due to args.save_total_limit
 49%|████▉     | 782/1600 [03:28<07:16,  1.87it/s] 49%|████▉     | 785/1600 [03:28<05:15,  2.58it/s] 49%|████▉     | 788/1600 [03:28<03:49,  3.54it/s] 49%|████▉     | 791/1600 [03:28<02:51,  4.72it/s] 50%|████▉     | 794/1600 [03:28<02:09,  6.22it/s] 50%|████▉     | 797/1600 [03:28<01:39,  8.05it/s] 50%|█████     | 800/1600 [03:29<01:17, 10.26it/s]                                                   50%|█████     | 800/1600 [03:29<01:17, 10.26it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.70it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.25it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.86it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.78it/s][A                                                  
                                               [A 50%|█████     | 800/1600 [03:29<01:17, 10.26it/s]
100%|██████████| 20/20 [00:00<00:00, 26.78it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-800
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-800/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-700] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-780] due to args.save_total_limit
 50%|█████     | 803/1600 [03:33<07:23,  1.80it/s] 50%|█████     | 806/1600 [03:34<05:19,  2.49it/s] 51%|█████     | 809/1600 [03:34<03:52,  3.40it/s] 51%|█████     | 812/1600 [03:34<02:51,  4.59it/s] 51%|█████     | 815/1600 [03:34<02:09,  6.08it/s] 51%|█████     | 818/1600 [03:34<01:39,  7.86it/s]                                                   51%|█████▏    | 820/1600 [03:34<01:39,  7.86it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.26it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.86it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.81it/s][A                                                  
                                               [A 51%|█████▏    | 820/1600 [03:35<01:39,  7.86it/s]
100%|██████████| 20/20 [00:00<00:00, 26.81it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-820
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-820/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-820/special_tokens_map.json
 51%|█████▏    | 821/1600 [03:38<06:57,  1.87it/s] 52%|█████▏    | 824/1600 [03:39<05:01,  2.57it/s] 52%|█████▏    | 827/1600 [03:39<03:40,  3.50it/s] 52%|█████▏    | 830/1600 [03:39<02:44,  4.69it/s] 52%|█████▏    | 833/1600 [03:39<02:02,  6.26it/s] 52%|█████▏    | 836/1600 [03:39<01:34,  8.09it/s] 52%|█████▏    | 839/1600 [03:39<01:14, 10.17it/s]                                                   52%|█████▎    | 840/1600 [03:39<01:14, 10.17it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.85it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.28it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.88it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.83it/s][A                                                  
                                               [A 52%|█████▎    | 840/1600 [03:40<01:14, 10.17it/s]
100%|██████████| 20/20 [00:00<00:00, 26.83it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-840
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-840/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-840/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-820] due to args.save_total_limit
 53%|█████▎    | 842/1600 [03:44<06:53,  1.83it/s] 53%|█████▎    | 845/1600 [03:44<04:58,  2.53it/s] 53%|█████▎    | 848/1600 [03:44<03:37,  3.46it/s] 53%|█████▎    | 851/1600 [03:44<02:41,  4.64it/s] 53%|█████▎    | 854/1600 [03:45<02:02,  6.10it/s] 54%|█████▎    | 857/1600 [03:45<01:33,  7.91it/s] 54%|█████▍    | 860/1600 [03:45<01:13, 10.10it/s]                                                   54%|█████▍    | 860/1600 [03:45<01:13, 10.10it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.60it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.87it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.23it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.80it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.68it/s][A                                                  
                                               [A 54%|█████▍    | 860/1600 [03:46<01:13, 10.10it/s]
100%|██████████| 20/20 [00:00<00:00, 26.68it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-860
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-860/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-860/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-840] due to args.save_total_limit
 54%|█████▍    | 863/1600 [03:49<06:34,  1.87it/s] 54%|█████▍    | 865/1600 [03:50<05:14,  2.34it/s] 54%|█████▍    | 868/1600 [03:50<03:41,  3.31it/s] 54%|█████▍    | 871/1600 [03:50<02:40,  4.54it/s] 55%|█████▍    | 874/1600 [03:50<02:00,  6.05it/s] 55%|█████▍    | 877/1600 [03:50<01:32,  7.86it/s] 55%|█████▌    | 880/1600 [03:50<01:11, 10.10it/s]                                                   55%|█████▌    | 880/1600 [03:50<01:11, 10.10it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.81it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.43it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.89it/s][A                                                  
                                               [A 55%|█████▌    | 880/1600 [03:51<01:11, 10.10it/s]
100%|██████████| 20/20 [00:00<00:00, 26.89it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-880
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-880/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-880/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-800] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-860] due to args.save_total_limit
 55%|█████▌    | 883/1600 [03:56<07:31,  1.59it/s] 55%|█████▌    | 885/1600 [03:56<05:56,  2.00it/s] 56%|█████▌    | 888/1600 [03:56<04:09,  2.85it/s] 56%|█████▌    | 891/1600 [03:56<03:00,  3.93it/s] 56%|█████▌    | 894/1600 [03:56<02:13,  5.31it/s] 56%|█████▌    | 897/1600 [03:56<01:40,  7.00it/s]                                                   56%|█████▋    | 900/1600 [03:56<01:40,  7.00it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.05it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.48it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.70it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.37it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.38it/s][A                                                  
                                               [A 56%|█████▋    | 900/1600 [03:57<01:40,  7.00it/s]
100%|██████████| 20/20 [00:00<00:00, 26.38it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-900/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-900/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-880] due to args.save_total_limit
 56%|█████▋    | 901/1600 [04:02<06:59,  1.67it/s] 56%|█████▋    | 904/1600 [04:02<05:07,  2.26it/s] 57%|█████▋    | 907/1600 [04:02<03:47,  3.05it/s] 57%|█████▋    | 910/1600 [04:02<02:48,  4.10it/s] 57%|█████▋    | 913/1600 [04:02<02:07,  5.39it/s] 57%|█████▋    | 916/1600 [04:02<01:37,  6.99it/s] 57%|█████▋    | 919/1600 [04:03<01:16,  8.94it/s]                                                   57%|█████▊    | 920/1600 [04:03<01:16,  8.94it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.22it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.49it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.82it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.45it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.42it/s][A                                                  
                                               [A 57%|█████▊    | 920/1600 [04:04<01:16,  8.94it/s]
100%|██████████| 20/20 [00:00<00:00, 26.42it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-920
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-920/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-920/special_tokens_map.json
 58%|█████▊    | 922/1600 [04:07<06:18,  1.79it/s] 58%|█████▊    | 925/1600 [04:07<04:33,  2.47it/s] 58%|█████▊    | 928/1600 [04:08<03:18,  3.38it/s] 58%|█████▊    | 931/1600 [04:08<02:27,  4.53it/s] 58%|█████▊    | 934/1600 [04:08<01:51,  5.97it/s] 59%|█████▊    | 937/1600 [04:08<01:26,  7.68it/s] 59%|█████▉    | 940/1600 [04:08<01:07,  9.79it/s]                                                   59%|█████▉    | 940/1600 [04:08<01:07,  9.79it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.57it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.91it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.29it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.80it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.64it/s][A                                                  
                                               [A 59%|█████▉    | 940/1600 [04:09<01:07,  9.79it/s]
100%|██████████| 20/20 [00:00<00:00, 26.64it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-940
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-940/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-940/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-920] due to args.save_total_limit
 59%|█████▉    | 943/1600 [04:13<06:06,  1.79it/s] 59%|█████▉    | 946/1600 [04:13<04:23,  2.48it/s] 59%|█████▉    | 949/1600 [04:13<03:11,  3.40it/s] 60%|█████▉    | 952/1600 [04:13<02:21,  4.58it/s] 60%|█████▉    | 955/1600 [04:13<01:46,  6.03it/s] 60%|█████▉    | 958/1600 [04:14<01:23,  7.72it/s]                                                   60%|██████    | 960/1600 [04:14<01:22,  7.72it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.65it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.94it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.28it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.82it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.66it/s][A                                                  
                                               [A 60%|██████    | 960/1600 [04:15<01:22,  7.72it/s]
100%|██████████| 20/20 [00:00<00:00, 26.66it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-960
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-960/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-960/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-940] due to args.save_total_limit
 60%|██████    | 961/1600 [04:18<06:06,  1.74it/s] 60%|██████    | 963/1600 [04:19<04:51,  2.18it/s] 60%|██████    | 966/1600 [04:19<03:25,  3.09it/s] 61%|██████    | 969/1600 [04:19<02:27,  4.27it/s] 61%|██████    | 972/1600 [04:19<01:50,  5.68it/s] 61%|██████    | 975/1600 [04:19<01:24,  7.38it/s] 61%|██████    | 978/1600 [04:19<01:06,  9.41it/s]                                                   61%|██████▏   | 980/1600 [04:19<01:05,  9.41it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.64it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.97it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.21it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.79it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.73it/s][A                                                  
                                               [A 61%|██████▏   | 980/1600 [04:20<01:05,  9.41it/s]
100%|██████████| 20/20 [00:00<00:00, 26.73it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-980
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-980/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-980/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-900] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-960] due to args.save_total_limit
 61%|██████▏   | 981/1600 [04:24<05:54,  1.75it/s] 61%|██████▏   | 983/1600 [04:24<04:41,  2.19it/s] 62%|██████▏   | 986/1600 [04:24<03:17,  3.11it/s] 62%|██████▏   | 989/1600 [04:24<02:22,  4.28it/s] 62%|██████▏   | 992/1600 [04:25<01:45,  5.78it/s] 62%|██████▏   | 995/1600 [04:25<01:20,  7.50it/s] 62%|██████▏   | 998/1600 [04:25<01:03,  9.51it/s]                                                   62%|██████▎   | 1000/1600 [04:25<01:03,  9.51it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.73it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.20it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.77it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.72it/s][A                                                   
                                               [A 62%|██████▎   | 1000/1600 [04:26<01:03,  9.51it/s]
100%|██████████| 20/20 [00:00<00:00, 26.72it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1000
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1000/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1000/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-980] due to args.save_total_limit
 63%|██████▎   | 1001/1600 [04:30<05:43,  1.74it/s] 63%|██████▎   | 1004/1600 [04:30<04:07,  2.41it/s] 63%|██████▎   | 1007/1600 [04:30<02:59,  3.31it/s] 63%|██████▎   | 1010/1600 [04:30<02:12,  4.46it/s] 63%|██████▎   | 1013/1600 [04:30<01:39,  5.91it/s] 64%|██████▎   | 1016/1600 [04:30<01:16,  7.62it/s] 64%|██████▎   | 1019/1600 [04:31<01:00,  9.61it/s]                                                    64%|██████▍   | 1020/1600 [04:31<01:00,  9.61it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.30it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.57it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.87it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.51it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.46it/s][A                                                   
                                               [A 64%|██████▍   | 1020/1600 [04:32<01:00,  9.61it/s]
100%|██████████| 20/20 [00:00<00:00, 26.46it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1020
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1020/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1020/special_tokens_map.json
 64%|██████▍   | 1022/1600 [04:36<05:33,  1.74it/s] 64%|██████▍   | 1025/1600 [04:36<03:59,  2.40it/s] 64%|██████▍   | 1028/1600 [04:36<02:54,  3.28it/s] 64%|██████▍   | 1031/1600 [04:36<02:08,  4.42it/s] 65%|██████▍   | 1034/1600 [04:36<01:36,  5.86it/s] 65%|██████▍   | 1037/1600 [04:36<01:14,  7.56it/s] 65%|██████▌   | 1040/1600 [04:36<00:57,  9.72it/s]                                                    65%|██████▌   | 1040/1600 [04:36<00:57,  9.72it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.06it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.48it/s][A
 60%|██████    | 12/20 [00:00<00:00, 28.83it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.48it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.41it/s][A                                                   
                                               [A 65%|██████▌   | 1040/1600 [04:37<00:57,  9.72it/s]
100%|██████████| 20/20 [00:00<00:00, 26.41it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1040
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1040/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1040/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1000] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1020] due to args.save_total_limit
 65%|██████▌   | 1043/1600 [04:42<05:36,  1.66it/s] 65%|██████▌   | 1046/1600 [04:42<04:01,  2.30it/s] 66%|██████▌   | 1049/1600 [04:42<02:54,  3.16it/s] 66%|██████▌   | 1052/1600 [04:42<02:08,  4.27it/s] 66%|██████▌   | 1055/1600 [04:42<01:36,  5.66it/s] 66%|██████▌   | 1058/1600 [04:42<01:13,  7.41it/s]                                                    66%|██████▋   | 1060/1600 [04:42<01:12,  7.41it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.62it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.87it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.20it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.80it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.76it/s][A                                                   
                                               [A 66%|██████▋   | 1060/1600 [04:43<01:12,  7.41it/s]
100%|██████████| 20/20 [00:00<00:00, 26.76it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1060
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1060/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1060/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1040] due to args.save_total_limit
 66%|██████▋   | 1061/1600 [04:47<04:59,  1.80it/s] 66%|██████▋   | 1064/1600 [04:47<03:35,  2.48it/s] 67%|██████▋   | 1066/1600 [04:47<02:53,  3.08it/s] 67%|██████▋   | 1069/1600 [04:47<02:03,  4.29it/s] 67%|██████▋   | 1072/1600 [04:47<01:31,  5.76it/s] 67%|██████▋   | 1075/1600 [04:48<01:08,  7.62it/s] 67%|██████▋   | 1078/1600 [04:48<00:53,  9.68it/s]                                                    68%|██████▊   | 1080/1600 [04:48<00:53,  9.68it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.81it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.09it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.42it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.83it/s][A                                                   
                                               [A 68%|██████▊   | 1080/1600 [04:49<00:53,  9.68it/s]
100%|██████████| 20/20 [00:00<00:00, 26.83it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1080
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1080/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1080/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1060] due to args.save_total_limit
 68%|██████▊   | 1081/1600 [04:52<04:39,  1.86it/s] 68%|██████▊   | 1084/1600 [04:52<03:20,  2.57it/s] 68%|██████▊   | 1087/1600 [04:52<02:25,  3.52it/s] 68%|██████▊   | 1090/1600 [04:53<01:47,  4.73it/s] 68%|██████▊   | 1093/1600 [04:53<01:21,  6.24it/s] 68%|██████▊   | 1096/1600 [04:53<01:02,  8.04it/s] 69%|██████▊   | 1099/1600 [04:53<00:49, 10.17it/s]                                                    69%|██████▉   | 1100/1600 [04:53<00:49, 10.17it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.79it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.03it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.22it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.83it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.78it/s][A                                                   
                                               [A 69%|██████▉   | 1100/1600 [04:54<00:49, 10.17it/s]
100%|██████████| 20/20 [00:00<00:00, 26.78it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1100
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1100/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1100/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1080] due to args.save_total_limit
 69%|██████▉   | 1102/1600 [04:58<04:27,  1.86it/s] 69%|██████▉   | 1105/1600 [04:58<03:12,  2.57it/s] 69%|██████▉   | 1108/1600 [04:58<02:20,  3.51it/s] 69%|██████▉   | 1111/1600 [04:58<01:43,  4.74it/s] 70%|██████▉   | 1114/1600 [04:58<01:17,  6.25it/s] 70%|██████▉   | 1117/1600 [04:58<01:00,  8.05it/s] 70%|███████   | 1120/1600 [04:58<00:46, 10.27it/s]                                                    70%|███████   | 1120/1600 [04:58<00:46, 10.27it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.82it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.42it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.89it/s][A                                                   
                                               [A 70%|███████   | 1120/1600 [04:59<00:46, 10.27it/s]
100%|██████████| 20/20 [00:00<00:00, 26.89it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1120
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1120/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1120/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1100] due to args.save_total_limit
 70%|███████   | 1123/1600 [05:03<04:13,  1.88it/s] 70%|███████   | 1126/1600 [05:03<03:01,  2.61it/s] 71%|███████   | 1129/1600 [05:03<02:12,  3.56it/s] 71%|███████   | 1132/1600 [05:03<01:37,  4.78it/s] 71%|███████   | 1135/1600 [05:03<01:14,  6.28it/s] 71%|███████   | 1138/1600 [05:04<00:56,  8.15it/s]                                                    71%|███████▏  | 1140/1600 [05:04<00:56,  8.15it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.82it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.93it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.76it/s][A                                                   
                                               [A 71%|███████▏  | 1140/1600 [05:05<00:56,  8.15it/s]
100%|██████████| 20/20 [00:00<00:00, 26.76it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1140
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1140/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1140/special_tokens_map.json
 71%|███████▏  | 1141/1600 [05:08<04:03,  1.89it/s] 72%|███████▏  | 1144/1600 [05:08<02:55,  2.60it/s] 72%|███████▏  | 1147/1600 [05:08<02:07,  3.55it/s] 72%|███████▏  | 1150/1600 [05:08<01:34,  4.77it/s] 72%|███████▏  | 1153/1600 [05:09<01:10,  6.30it/s] 72%|███████▏  | 1156/1600 [05:09<00:54,  8.12it/s] 72%|███████▏  | 1159/1600 [05:09<00:43, 10.12it/s]                                                    72%|███████▎  | 1160/1600 [05:09<00:43, 10.12it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.62it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.90it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.27it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.83it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.69it/s][A                                                   
                                               [A 72%|███████▎  | 1160/1600 [05:10<00:43, 10.12it/s]
100%|██████████| 20/20 [00:00<00:00, 26.69it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1160
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1160/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1160/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1140] due to args.save_total_limit
 73%|███████▎  | 1162/1600 [05:13<03:52,  1.89it/s] 73%|███████▎  | 1165/1600 [05:14<02:46,  2.61it/s] 73%|███████▎  | 1168/1600 [05:14<02:01,  3.57it/s] 73%|███████▎  | 1171/1600 [05:14<01:29,  4.79it/s] 73%|███████▎  | 1174/1600 [05:14<01:07,  6.30it/s] 74%|███████▎  | 1177/1600 [05:14<00:52,  8.13it/s] 74%|███████▍  | 1180/1600 [05:14<00:40, 10.29it/s]                                                    74%|███████▍  | 1180/1600 [05:14<00:40, 10.29it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.92it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.75it/s][A                                                   
                                               [A 74%|███████▍  | 1180/1600 [05:15<00:40, 10.29it/s]
100%|██████████| 20/20 [00:00<00:00, 26.75it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1180
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1180/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1180/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1160] due to args.save_total_limit
 74%|███████▍  | 1183/1600 [05:19<03:44,  1.86it/s] 74%|███████▍  | 1186/1600 [05:19<02:41,  2.57it/s] 74%|███████▍  | 1189/1600 [05:19<01:57,  3.51it/s] 74%|███████▍  | 1192/1600 [05:19<01:27,  4.69it/s] 75%|███████▍  | 1195/1600 [05:19<01:05,  6.20it/s] 75%|███████▍  | 1198/1600 [05:19<00:50,  7.99it/s]                                                    75%|███████▌  | 1200/1600 [05:20<00:50,  7.99it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.75it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.78it/s][A                                                   
                                               [A 75%|███████▌  | 1200/1600 [05:20<00:50,  7.99it/s]
100%|██████████| 20/20 [00:00<00:00, 26.78it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1200
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1200/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1200/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1120] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1180] due to args.save_total_limit
 75%|███████▌  | 1201/1600 [05:24<03:43,  1.78it/s] 75%|███████▌  | 1204/1600 [05:24<02:40,  2.47it/s] 75%|███████▌  | 1207/1600 [05:24<01:56,  3.38it/s] 76%|███████▌  | 1210/1600 [05:25<01:25,  4.56it/s] 76%|███████▌  | 1213/1600 [05:25<01:04,  6.00it/s] 76%|███████▌  | 1216/1600 [05:25<00:49,  7.78it/s] 76%|███████▌  | 1219/1600 [05:25<00:39,  9.70it/s]                                                    76%|███████▋  | 1220/1600 [05:25<00:39,  9.70it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.87it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.76it/s][A                                                   
                                               [A 76%|███████▋  | 1220/1600 [05:26<00:39,  9.70it/s]
100%|██████████| 20/20 [00:00<00:00, 26.76it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1220
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1220/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1220/special_tokens_map.json
 76%|███████▋  | 1222/1600 [05:30<03:19,  1.89it/s] 77%|███████▋  | 1225/1600 [05:30<02:23,  2.61it/s] 77%|███████▋  | 1228/1600 [05:30<01:44,  3.55it/s] 77%|███████▋  | 1231/1600 [05:30<01:17,  4.78it/s] 77%|███████▋  | 1234/1600 [05:30<00:58,  6.27it/s] 77%|███████▋  | 1237/1600 [05:30<00:44,  8.21it/s]                                                    78%|███████▊  | 1240/1600 [05:30<00:43,  8.21it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.79it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.39it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.92it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.73it/s][A                                                   
                                               [A 78%|███████▊  | 1240/1600 [05:31<00:43,  8.21it/s]
100%|██████████| 20/20 [00:00<00:00, 26.73it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1240
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1240/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1240/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1200] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1220] due to args.save_total_limit
 78%|███████▊  | 1241/1600 [05:35<03:04,  1.94it/s] 78%|███████▊  | 1244/1600 [05:35<02:16,  2.61it/s] 78%|███████▊  | 1247/1600 [05:35<01:40,  3.50it/s] 78%|███████▊  | 1250/1600 [05:35<01:15,  4.64it/s] 78%|███████▊  | 1253/1600 [05:35<00:56,  6.09it/s] 78%|███████▊  | 1256/1600 [05:36<00:43,  7.82it/s] 79%|███████▊  | 1259/1600 [05:36<00:34,  9.82it/s]                                                    79%|███████▉  | 1260/1600 [05:36<00:34,  9.82it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.81it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.87it/s][A                                                   
                                               [A 79%|███████▉  | 1260/1600 [05:37<00:34,  9.82it/s]
100%|██████████| 20/20 [00:00<00:00, 26.87it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1260
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1260/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1260/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1240] due to args.save_total_limit
 79%|███████▉  | 1262/1600 [05:40<02:59,  1.88it/s] 79%|███████▉  | 1265/1600 [05:40<02:09,  2.59it/s] 79%|███████▉  | 1268/1600 [05:41<01:34,  3.52it/s] 79%|███████▉  | 1271/1600 [05:41<01:09,  4.73it/s] 80%|███████▉  | 1274/1600 [05:41<00:52,  6.22it/s] 80%|███████▉  | 1277/1600 [05:41<00:40,  8.02it/s] 80%|████████  | 1280/1600 [05:41<00:31, 10.25it/s]                                                    80%|████████  | 1280/1600 [05:41<00:31, 10.25it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.78it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.03it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.23it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.80it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.68it/s][A                                                   
                                               [A 80%|████████  | 1280/1600 [05:42<00:31, 10.25it/s]
100%|██████████| 20/20 [00:00<00:00, 26.68it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1280
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1280/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1280/special_tokens_map.json
 80%|████████  | 1283/1600 [05:46<02:42,  1.95it/s] 80%|████████  | 1286/1600 [05:46<01:56,  2.69it/s] 81%|████████  | 1289/1600 [05:46<01:25,  3.66it/s] 81%|████████  | 1292/1600 [05:46<01:02,  4.90it/s] 81%|████████  | 1295/1600 [05:46<00:47,  6.46it/s] 81%|████████  | 1298/1600 [05:46<00:36,  8.24it/s]                                                    81%|████████▏ | 1300/1600 [05:46<00:36,  8.24it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 29.99it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.37it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.90it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.74it/s][A                                                   
                                               [A 81%|████████▏ | 1300/1600 [05:47<00:36,  8.24it/s]
100%|██████████| 20/20 [00:00<00:00, 26.74it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1300
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1300/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1300/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1280] due to args.save_total_limit
 81%|████████▏ | 1301/1600 [05:51<02:44,  1.82it/s] 82%|████████▏ | 1304/1600 [05:51<01:57,  2.51it/s] 82%|████████▏ | 1307/1600 [05:51<01:25,  3.43it/s] 82%|████████▏ | 1310/1600 [05:51<01:02,  4.63it/s] 82%|████████▏ | 1313/1600 [05:51<00:46,  6.14it/s] 82%|████████▏ | 1316/1600 [05:51<00:36,  7.89it/s] 82%|████████▏ | 1319/1600 [05:52<00:28,  9.84it/s]                                                    82%|████████▎ | 1320/1600 [05:52<00:28,  9.84it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.77it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.96it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.82it/s][A                                                   
                                               [A 82%|████████▎ | 1320/1600 [05:53<00:28,  9.84it/s]
100%|██████████| 20/20 [00:00<00:00, 26.82it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1320
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1320/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1320/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1300] due to args.save_total_limit
 83%|████████▎ | 1322/1600 [05:57<02:38,  1.76it/s] 83%|████████▎ | 1325/1600 [05:57<01:53,  2.43it/s] 83%|████████▎ | 1328/1600 [05:57<01:21,  3.33it/s] 83%|████████▎ | 1331/1600 [05:57<00:59,  4.49it/s] 83%|████████▎ | 1334/1600 [05:57<00:44,  5.96it/s] 84%|████████▎ | 1337/1600 [05:57<00:34,  7.71it/s] 84%|████████▍ | 1340/1600 [05:57<00:26,  9.90it/s]                                                    84%|████████▍ | 1340/1600 [05:57<00:26,  9.90it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.23it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.84it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.80it/s][A                                                   
                                               [A 84%|████████▍ | 1340/1600 [05:58<00:26,  9.90it/s]
100%|██████████| 20/20 [00:00<00:00, 26.80it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1340
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1340/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1340/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1260] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1320] due to args.save_total_limit
 84%|████████▍ | 1343/1600 [06:02<02:21,  1.81it/s] 84%|████████▍ | 1346/1600 [06:02<01:41,  2.50it/s] 84%|████████▍ | 1349/1600 [06:02<01:13,  3.41it/s] 84%|████████▍ | 1352/1600 [06:02<00:53,  4.61it/s] 85%|████████▍ | 1355/1600 [06:03<00:40,  6.08it/s] 85%|████████▍ | 1358/1600 [06:03<00:30,  7.87it/s]                                                    85%|████████▌ | 1360/1600 [06:03<00:30,  7.87it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.75it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.01it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.36it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.82it/s][A                                                   
                                               [A 85%|████████▌ | 1360/1600 [06:04<00:30,  7.87it/s]
100%|██████████| 20/20 [00:00<00:00, 26.82it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1360
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1360/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1360/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1340] due to args.save_total_limit
 85%|████████▌ | 1361/1600 [06:07<02:10,  1.82it/s] 85%|████████▌ | 1364/1600 [06:07<01:33,  2.52it/s] 85%|████████▌ | 1367/1600 [06:08<01:07,  3.45it/s] 86%|████████▌ | 1370/1600 [06:08<00:49,  4.64it/s] 86%|████████▌ | 1373/1600 [06:08<00:37,  6.12it/s] 86%|████████▌ | 1376/1600 [06:08<00:28,  7.90it/s] 86%|████████▌ | 1379/1600 [06:08<00:22,  9.88it/s]                                                    86%|████████▋ | 1380/1600 [06:08<00:22,  9.88it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.03it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.37it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.93it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.84it/s][A                                                   
                                               [A 86%|████████▋ | 1380/1600 [06:09<00:22,  9.88it/s]
100%|██████████| 20/20 [00:00<00:00, 26.84it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1380
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1380/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1380/special_tokens_map.json
 86%|████████▋ | 1382/1600 [06:13<01:53,  1.92it/s] 87%|████████▋ | 1385/1600 [06:13<01:21,  2.64it/s] 87%|████████▋ | 1388/1600 [06:13<00:58,  3.62it/s] 87%|████████▋ | 1391/1600 [06:13<00:43,  4.85it/s] 87%|████████▋ | 1394/1600 [06:13<00:32,  6.42it/s] 87%|████████▋ | 1397/1600 [06:13<00:24,  8.25it/s] 88%|████████▊ | 1400/1600 [06:13<00:19, 10.48it/s]                                                    88%|████████▊ | 1400/1600 [06:13<00:19, 10.48it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.76it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.04it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.37it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.80it/s][A                                                   
                                               [A 88%|████████▊ | 1400/1600 [06:14<00:19, 10.48it/s]
100%|██████████| 20/20 [00:00<00:00, 26.80it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1400
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1400/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1400/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1360] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1380] due to args.save_total_limit
 88%|████████▊ | 1403/1600 [06:18<01:46,  1.84it/s] 88%|████████▊ | 1406/1600 [06:18<01:16,  2.54it/s] 88%|████████▊ | 1409/1600 [06:18<00:54,  3.48it/s] 88%|████████▊ | 1412/1600 [06:18<00:40,  4.67it/s] 88%|████████▊ | 1415/1600 [06:19<00:30,  6.15it/s] 89%|████████▊ | 1418/1600 [06:19<00:22,  7.94it/s]                                                    89%|████████▉ | 1420/1600 [06:19<00:22,  7.94it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.79it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.00it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.17it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.67it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.53it/s][A                                                   
                                               [A 89%|████████▉ | 1420/1600 [06:20<00:22,  7.94it/s]
100%|██████████| 20/20 [00:00<00:00, 26.53it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1420
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1420/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1420/special_tokens_map.json
 89%|████████▉ | 1421/1600 [06:23<01:36,  1.85it/s] 89%|████████▉ | 1424/1600 [06:23<01:08,  2.56it/s] 89%|████████▉ | 1427/1600 [06:23<00:49,  3.50it/s] 89%|████████▉ | 1430/1600 [06:24<00:36,  4.72it/s] 90%|████████▉ | 1433/1600 [06:24<00:27,  6.18it/s] 90%|████████▉ | 1436/1600 [06:24<00:20,  7.97it/s] 90%|████████▉ | 1439/1600 [06:24<00:15, 10.09it/s]                                                    90%|█████████ | 1440/1600 [06:24<00:15, 10.09it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.94it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.76it/s][A                                                   
                                               [A 90%|█████████ | 1440/1600 [06:25<00:15, 10.09it/s]
100%|██████████| 20/20 [00:00<00:00, 26.76it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1440
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1440/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1440/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1400] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1420] due to args.save_total_limit
 90%|█████████ | 1442/1600 [06:29<01:27,  1.81it/s] 90%|█████████ | 1445/1600 [06:29<01:01,  2.51it/s] 90%|█████████ | 1448/1600 [06:29<00:44,  3.44it/s] 91%|█████████ | 1451/1600 [06:29<00:32,  4.63it/s] 91%|█████████ | 1454/1600 [06:29<00:23,  6.10it/s] 91%|█████████ | 1457/1600 [06:29<00:18,  7.82it/s] 91%|█████████▏| 1460/1600 [06:29<00:13, 10.04it/s]                                                    91%|█████████▏| 1460/1600 [06:30<00:13, 10.04it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.80it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.25it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.80it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.66it/s][A                                                   
                                               [A 91%|█████████▏| 1460/1600 [06:30<00:13, 10.04it/s]
100%|██████████| 20/20 [00:00<00:00, 26.66it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1460
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1460/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1460/special_tokens_map.json
 91%|█████████▏| 1463/1600 [06:34<01:12,  1.90it/s] 92%|█████████▏| 1466/1600 [06:34<00:51,  2.61it/s] 92%|█████████▏| 1469/1600 [06:34<00:36,  3.59it/s] 92%|█████████▏| 1472/1600 [06:34<00:26,  4.84it/s] 92%|█████████▏| 1475/1600 [06:35<00:19,  6.37it/s] 92%|█████████▏| 1478/1600 [06:35<00:15,  8.07it/s]                                                    92%|█████████▎| 1480/1600 [06:35<00:14,  8.07it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.77it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.00it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.18it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.75it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.63it/s][A                                                   
                                               [A 92%|█████████▎| 1480/1600 [06:36<00:14,  8.07it/s]
100%|██████████| 20/20 [00:00<00:00, 26.63it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1480
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1480/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1480/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1460] due to args.save_total_limit
 93%|█████████▎| 1481/1600 [06:39<01:05,  1.82it/s] 93%|█████████▎| 1484/1600 [06:39<00:46,  2.50it/s] 93%|█████████▎| 1487/1600 [06:40<00:33,  3.42it/s] 93%|█████████▎| 1490/1600 [06:40<00:23,  4.60it/s] 93%|█████████▎| 1493/1600 [06:40<00:17,  6.13it/s] 94%|█████████▎| 1496/1600 [06:40<00:13,  7.91it/s] 94%|█████████▎| 1499/1600 [06:40<00:10, 10.03it/s]                                                    94%|█████████▍| 1500/1600 [06:40<00:09, 10.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.79it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.07it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.40it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.91it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.83it/s][A                                                   
                                               [A 94%|█████████▍| 1500/1600 [06:41<00:09, 10.03it/s]
100%|██████████| 20/20 [00:00<00:00, 26.83it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1500
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1500/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1500/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1480] due to args.save_total_limit
 94%|█████████▍| 1502/1600 [06:45<00:52,  1.85it/s] 94%|█████████▍| 1505/1600 [06:45<00:37,  2.56it/s] 94%|█████████▍| 1508/1600 [06:45<00:26,  3.49it/s] 94%|█████████▍| 1511/1600 [06:45<00:18,  4.73it/s] 95%|█████████▍| 1514/1600 [06:45<00:13,  6.24it/s] 95%|█████████▍| 1517/1600 [06:45<00:10,  8.02it/s] 95%|█████████▌| 1520/1600 [06:45<00:07, 10.24it/s]                                                    95%|█████████▌| 1520/1600 [06:46<00:07, 10.24it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.75it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.01it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.22it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.73it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.61it/s][A                                                   
                                               [A 95%|█████████▌| 1520/1600 [06:46<00:07, 10.24it/s]
100%|██████████| 20/20 [00:00<00:00, 26.61it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1520
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1520/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1520/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1500] due to args.save_total_limit
 95%|█████████▌| 1523/1600 [06:50<00:40,  1.89it/s] 95%|█████████▌| 1526/1600 [06:50<00:28,  2.60it/s] 96%|█████████▌| 1529/1600 [06:50<00:19,  3.55it/s] 96%|█████████▌| 1532/1600 [06:50<00:14,  4.79it/s] 96%|█████████▌| 1535/1600 [06:51<00:10,  6.26it/s] 96%|█████████▌| 1538/1600 [06:51<00:07,  8.05it/s]                                                    96%|█████████▋| 1540/1600 [06:51<00:07,  8.05it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.87it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.08it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.43it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.89it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.73it/s][A                                                   
                                               [A 96%|█████████▋| 1540/1600 [06:52<00:07,  8.05it/s]
100%|██████████| 20/20 [00:00<00:00, 26.73it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1540
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1540/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1540/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1520] due to args.save_total_limit
 96%|█████████▋| 1541/1600 [06:56<00:33,  1.74it/s] 96%|█████████▋| 1544/1600 [06:56<00:23,  2.41it/s] 97%|█████████▋| 1547/1600 [06:56<00:16,  3.30it/s] 97%|█████████▋| 1550/1600 [06:56<00:11,  4.45it/s] 97%|█████████▋| 1553/1600 [06:56<00:07,  5.90it/s] 97%|█████████▋| 1556/1600 [06:56<00:05,  7.69it/s] 97%|█████████▋| 1559/1600 [06:56<00:04,  9.79it/s]                                                    98%|█████████▊| 1560/1600 [06:56<00:04,  9.79it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.76it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.06it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.41it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.98it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.88it/s][A                                                   
                                               [A 98%|█████████▊| 1560/1600 [06:57<00:04,  9.79it/s]
100%|██████████| 20/20 [00:00<00:00, 26.88it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1560
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1560/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1560/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1540] due to args.save_total_limit
 98%|█████████▊| 1562/1600 [07:01<00:20,  1.87it/s] 98%|█████████▊| 1565/1600 [07:01<00:13,  2.57it/s] 98%|█████████▊| 1568/1600 [07:01<00:09,  3.51it/s] 98%|█████████▊| 1571/1600 [07:01<00:06,  4.74it/s] 98%|█████████▊| 1574/1600 [07:01<00:04,  6.21it/s] 99%|█████████▊| 1577/1600 [07:02<00:02,  8.03it/s] 99%|█████████▉| 1580/1600 [07:02<00:01, 10.19it/s]                                                    99%|█████████▉| 1580/1600 [07:02<00:01, 10.19it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.77it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.01it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.33it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.88it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.73it/s][A                                                   
                                               [A 99%|█████████▉| 1580/1600 [07:03<00:01, 10.19it/s]
100%|██████████| 20/20 [00:00<00:00, 26.73it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1580
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1580/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1580/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1440] due to args.save_total_limit
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1560] due to args.save_total_limit
 99%|█████████▉| 1583/1600 [07:07<00:09,  1.79it/s] 99%|█████████▉| 1586/1600 [07:07<00:05,  2.47it/s] 99%|█████████▉| 1589/1600 [07:07<00:03,  3.39it/s]100%|█████████▉| 1592/1600 [07:07<00:01,  4.57it/s]100%|█████████▉| 1595/1600 [07:07<00:00,  6.05it/s]100%|█████████▉| 1598/1600 [07:07<00:00,  7.81it/s]                                                   100%|██████████| 1600/1600 [07:07<00:00,  7.81it/s]The following columns in the evaluation set  don't have a corresponding argument in `M2M100.forward` and have been ignored: hypothesis, premise. If hypothesis, premise are not expected by `M2M100.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2490
  Batch size = 128

  0%|          | 0/20 [00:00<?, ?it/s][A
 20%|██        | 4/20 [00:00<00:00, 35.77it/s][A
 40%|████      | 8/20 [00:00<00:00, 30.05it/s][A
 60%|██████    | 12/20 [00:00<00:00, 29.39it/s][A
 75%|███████▌  | 15/20 [00:00<00:00, 27.88it/s][A
 90%|█████████ | 18/20 [00:00<00:00, 26.73it/s][A                                                   
                                               [A100%|██████████| 1600/1600 [07:08<00:00,  7.81it/s]
100%|██████████| 20/20 [00:00<00:00, 26.73it/s][A
                                               [ASaving model checkpoint to /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1600
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1600/tokenizer_config.json
Special tokens file saved in /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1600/special_tokens_map.json
Deleting older checkpoint [/home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1580] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from /home/bhanuv/projects/classification_checkpoints/m2m100_0/checkpoint-1600 (score: 1.0470449924468994).
                                                   100%|██████████| 1600/1600 [07:12<00:00,  7.81it/s]100%|██████████| 1600/1600 [07:12<00:00,  3.70it/s]
