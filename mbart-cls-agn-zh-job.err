Using custom data configuration default-language=zh
Reusing dataset xnli (/home/bhanuv/.cache/huggingface/datasets/xnli/default-language=zh/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd)
Some weights of the model checkpoint at facebook/mbart-large-50-many-to-many-mmt were not used when initializing MBartModel: ['final_logits_bias', 'lm_head.weight']
- This IS expected if you are initializing MBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading cached processed dataset at /home/bhanuv/.cache/huggingface/datasets/xnli/default-language=zh/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd/cache-dd5fd341c8bdc576.arrow
Using amp half precision backend
The following columns in the test set  don't have a corresponding argument in `MBart.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `MBart.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5010
  Batch size = 128
  0%|          | 0/40 [00:00<?, ?it/s] 10%|█         | 4/40 [00:00<00:01, 32.53it/s] 20%|██        | 8/40 [00:00<00:01, 25.56it/s] 28%|██▊       | 11/40 [00:00<00:01, 23.53it/s] 35%|███▌      | 14/40 [00:00<00:01, 21.46it/s] 42%|████▎     | 17/40 [00:00<00:01, 21.55it/s] 50%|█████     | 20/40 [00:00<00:00, 22.36it/s] 57%|█████▊    | 23/40 [00:01<00:00, 22.10it/s] 65%|██████▌   | 26/40 [00:01<00:00, 21.74it/s] 72%|███████▎  | 29/40 [00:01<00:00, 21.35it/s] 80%|████████  | 32/40 [00:01<00:00, 21.00it/s] 88%|████████▊ | 35/40 [00:01<00:00, 19.91it/s] 95%|█████████▌| 38/40 [00:01<00:00, 18.87it/s]100%|██████████| 40/40 [00:02<00:00, 19.32it/s]